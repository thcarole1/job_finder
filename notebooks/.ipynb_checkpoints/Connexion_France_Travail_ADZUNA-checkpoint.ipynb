{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d621d-ffac-4fe6-8e18-07c055e9d366",
   "metadata": {},
   "source": [
    "# Connexion_France_Travail_ADZUNA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e368b-be9e-4195-b2ab-5f3eebcdcc12",
   "metadata": {},
   "source": [
    "Ce script a pour objectif :\n",
    "- d'extraire les offres d'emploi mises à disposition par :\n",
    "  <br> _ l'API de **France Travail**\n",
    "  <br>_ de l'API **ADZUNA**\n",
    "- les stocker dans :\n",
    "  <br> _ un fichier (**CSV**) en local\n",
    "  <br> _ un fichier (**Parquet**) en local\n",
    "  <br> _ dans une **BDD PostgreSQL** en local.\n",
    "\n",
    "**/!\\ Ajouts !** : \n",
    "- Modification de la fonction \"Main\" : Boucle 'for' pour exécuter le programme pour plusieurs métiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129f84d-2d01-44eb-ba17-25d4cf673d3c",
   "metadata": {},
   "source": [
    "Comment ?\n",
    "1. Sur la base de critères spécifiques (mots clés, localisation, etc...), \n",
    "    - lancement d'une requête pour obtenir les offres d'emploi correspondantes via l'API France Travail\n",
    "    - lancement d'une requête pour obtenir les offres d'emploi correspondantes via l'API Adzuna\n",
    "2. Une fois les offres trouvées, vérification et suppression des doublons.\n",
    "3. Une sauvegarde en local des offres sont stockées dans un fichier (**CSV**).\n",
    "4. Une sauvegarde en local des offres sont stockées dans un fichier (**Parquet**).\n",
    "5. Une sauvegarde dans une base de données **PostgreSQL** est également effectuée en local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6874620-7676-4138-9b30-0ac53198d8f1",
   "metadata": {},
   "source": [
    "Les URL (FRANCE TRAVAIL) utiles sont :\n",
    "- https://francetravail.io/data/api/offres-emploi\n",
    "- https://francetravail.io/data/api/offres-emploi/documentation#/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736e6c8-5d60-47e3-8eb8-cbf900de62c1",
   "metadata": {},
   "source": [
    "Les URL (ADZUNA) utiles sont :\n",
    "- https://developer.adzuna.com/overview\n",
    "- https://developer.adzuna.com/docs/search\n",
    "- https://developer.adzuna.com/activedocs#!/adzuna/search\n",
    "- https://developer.adzuna.com/overview\n",
    "- https://www.adzuna.fr/details/5376850320?utm_medium=api&utm_source=6d1ef246"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b6efe-f320-4b0d-86ec-44109e1620b6",
   "metadata": {},
   "source": [
    "URL utile pour récupérer les informations géographiques:\n",
    "- https://www.data.gouv.fr/datasets/contours-communes-france-administrative-format-admin-express-avec-arrondissements/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77453b-9814-4567-97aa-7f680bfdfe9b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1566,
   "id": "59e94172-c17e-4a12-9519-d1b26cd600df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Table, MetaData, text\n",
    "from sqlalchemy.dialects.postgresql import insert as pg_insert\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import http.client\n",
    "import json\n",
    "import hashlib\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dd34f-6c24-4f94-b985-40336f97d531",
   "metadata": {},
   "source": [
    "## Procédure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d771d39-0166-45aa-abda-90d58d82f440",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e18cf1-1961-4200-9bb9-497ce803db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  VARIABLES  ##################\n",
    "# France Travail\n",
    "FT_CLIENT_ID = os.environ.get(\"FT_CLIENT_ID\")\n",
    "FT_CLIENT_SECRET = os.environ.get(\"FT_CLIENT_SECRET\")\n",
    "FT_SCOPE = os.environ.get(\"FT_SCOPE\")\n",
    "\n",
    "# Adzuna\n",
    "ADZUNA_CLIENT_ID = os.environ.get(\"ADZUNA_CLIENT_ID\")\n",
    "ADZUNA_CLIENT_SECRET = os.environ.get(\"ADZUNA_CLIENT_SECRET\")\n",
    "\n",
    "# Param Database PostgreSQL\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"jobsdb\")\n",
    "DB_USER = os.environ.get(\"DB_USER\",\"jobsuser\")\n",
    "DB_PASS = os.environ.get(\"DB_PASS\", \"jobspass\")\n",
    "DB_HOST = os.environ.get(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\",\"5432\")\n",
    "\n",
    "# Nom table\n",
    "DB_TABLE_NAME = \"offres\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f0bec-9475-4dd1-92d6-f6a21a487f7e",
   "metadata": {},
   "source": [
    "### Configuration Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f970c-1849-454a-8191-ebe47ac0008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(LOG_DIR, f\"pipeline_{datetime.now().strftime('%Y-%m-%d')}.log\"),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7dbbdb-2ef2-494a-ad27-31bb79f36ba1",
   "metadata": {},
   "source": [
    "### NLP ET embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f627bbe-919e-48bf-917d-0f50965505a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP & embeddings\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c6b29-9af0-4439-a594-b5527a2dee1d",
   "metadata": {},
   "source": [
    "### Texte de référence (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f34d4e-8de1-45c8-a307-edcd2505f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offre de référence\n",
    "reference_text = \"\"\"\n",
    "-\tData Analyst\n",
    "-\tData Scientist\n",
    "-\tData Analyst en reconversion\n",
    "-\t10 ans d’expérience industrie automobile & achats\n",
    "-\tExpert en dashboards et optimisation de performance\n",
    "\n",
    "-\tLocalisation :\n",
    "o\tIle-de-France\n",
    "o\tYvelines\n",
    "o\tPoissy\n",
    "o\t78\n",
    "\n",
    "-\tCompétences :\n",
    "o\tData Analysis & BI : Power BI (DAX, Power Query), Excel avancé, SQL.\n",
    "o\tData Visualization : Création de tableaux de bord et KPIs pour la prise de décision.\n",
    "o\tStatistiques & Prévisions : Analyses quantitatives, modèles prédictifs, préventions des risques.\n",
    "o\tConception d’outils d’aide à la décision et de tableaux de bord stratégiques\n",
    "o\tMéthodologies : Gestion de projets analytiques, reporting automatisé.\n",
    "o\tExploitation de solutions de Data Science & Intelligence Artificielle : Machine Learning, modèles prédictifs, classification, régression, clustering\n",
    "o\tAnalyses et modélisations statistiques avancées : Python, outils BI, Dataiku\n",
    "o\tGestion et structuration de données massives : SQL Server, MySQL, Cloud Azure (Machine Learning Studio).\n",
    "o\tDéveloppement et optimisation d’algorithmes : pour la performance et l’automatisation.\n",
    "o\tConception d’outils d’aide à la décision et de tableaux de bord stratégiques pour orienter les choix business.\n",
    "\n",
    "-\tInformatique :\n",
    "o\tLangages de programmation : Python, Java, SQL\n",
    "o\tLogiciel : Power BI, Excel, Dataiku, Jupyter Notebook\n",
    "o\tCloud : Azure, Google Cloud Platform\n",
    "\n",
    "-\tDiplômes et Formations :\n",
    "o\tCertification Microsoft Analyste de Données Power BI (PL300)\n",
    "o\tBac+4 - Concepteur développeur en IA et analyse Big Data\n",
    "o\tBac+5 –Master 2 Electronique Electrotechnique et Automatique\n",
    "\n",
    "-\tAtouts :\n",
    "o\tAutonome et rigoureux dans la gestion de projets.\n",
    "o\tVulgarise des résultats complexes pour des non-spécialistes.\n",
    "o\tOrganise et priorise les tâches orientées résultats.\n",
    "o\tEn veille active sur l’IA et les nouvelles technologies.\n",
    "o\tCurieux\n",
    "o\tAutonome et rigoureux\n",
    "o\tForce de proposition\n",
    "o\tA l'écoute\n",
    "o\tEsprit d'équipe\n",
    "-\tLangues : Anglais courant\n",
    "-\tExpériences professionnelles : \n",
    "o\tAcheteur de composants : \n",
    "\tAnalyser et structurer des données fournisseurs pour l’optimisation des coûts.\n",
    "\tDévelopper des tableaux de bord (KPI, suivi de performance) automatisés.\n",
    "\tCommuniquer des insights aux équipes finance, qualité et production.\n",
    "\tRéaliser des économies supérieures de 20 % aux objectifs fixés.\n",
    "\tAnalyser et structurer des données massives pour optimiser les coûts et la performance fournisseurs.\n",
    "\tCréer et automatiser de tableaux de bord (Power BI, Excel avancé) pour le suivi des KPIs.\n",
    "\tDévelopper des modèles prédictifs pour la prévision des coûts et l’analyse de tendances.\n",
    "\tGérer des projets interfonctionnels (production, finance, qualité), générant +20 % d’économies au-delà des objectifs.\n",
    "o\tResponsable de développement de machines électrique :\n",
    "\tAnalyser et valider des données issues des tests de performance produits.\n",
    "\tAutomatiser des traitements statistiques pour réduire les erreurs de reporting.\n",
    "\tMettre en place de modèles prédictifs pour améliorer la fiabilité des composants.\n",
    "\tConcevoir des solutions analytiques pour optimiser la durabilité et la performance des composants.\n",
    "\tCollaborer en mode Agile avec équipes R&D et Data pour intégrer l’analyse dans l’amélioration continue.\n",
    "\tAnalyse statistique et validation de données issues de tests de performance.\n",
    "-\tCentres d’intérêt :\n",
    "o\tThéâtre : Improvisation\n",
    "o\tMoto : Sorties en groupe\n",
    "\"\"\"\n",
    "reference_text_clean = \" \".join([token.lemma_ for token in nlp(reference_text.lower()) if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9df97e-5380-49d2-b091-d1e2e5886be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Offre de référence\n",
    "# reference_text = \"\"\"\n",
    "# Data Analyst avec expertise Python, SQL, Power BI et analyse de données industrielles.\n",
    "# \"\"\"\n",
    "# reference_text_clean = \" \".join([token.lemma_ for token in nlp(reference_text.lower()) if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631ac5b-b706-486a-bea2-e47ef6a4f9a4",
   "metadata": {},
   "source": [
    "### Paramètres de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47f4fc-a8ce-482c-ae6d-250061c5682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# PARAMETRES DE RECHERCHE\n",
    "# ---------------------------\n",
    "\n",
    "# Paramètres de recherche\n",
    "# JOB_QUERY = \"data analyst\"\n",
    "COMMUNE = \"78300\"\n",
    "DISTANCE = 100000\n",
    "\n",
    "# Nombre d'annonces par page requise\n",
    "BLOC_PAGINATION = 50\n",
    "\n",
    "# Nombre de pages max\n",
    "MAX_PAGES = 20   # Limiter le nombre de pages récupérées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c2baf-0c81-4879-9cbb-7d3cad0f33a0",
   "metadata": {},
   "source": [
    "### Paramètres de sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13daef8d-56fc-4538-994e-cecd5e78e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# PARAMETRES DE SAUVEGARDE\n",
    "# ---------------------------\n",
    "# Répertoires\n",
    "# Raw_data\n",
    "CSV_DIR_RAW = \"../data/raw_data/csv\"\n",
    "PARQUET_DIR_RAW = \"../data/raw_data/parquet\"\n",
    "\n",
    "# Processed_data\n",
    "CSV_DIR_PROC = \"../data/processed_data/csv\"\n",
    "PARQUET_DIR_PROC = \"../data/processed_data/parquet\"\n",
    "EXCEL_DIR_PROC = \"../data/processed_data/excel\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b50db3-037e-4050-b095-393fd8ca5aa1",
   "metadata": {},
   "source": [
    "### Authentification France Travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af447536-bcf7-4def-8b46-a45c510ff850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# AUTH FRANCE TRAVAIL\n",
    "# ---------------------------\n",
    "def get_ft_token(retries=3, wait=5):\n",
    "    url = \"https://entreprise.pole-emploi.fr/connexion/oauth2/access_token?realm=/partenaire\"\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": FT_CLIENT_ID,\n",
    "        \"client_secret\": FT_CLIENT_SECRET,\n",
    "        \"scope\": FT_SCOPE,\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = requests.post(url, data=data)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"access_token\"]\n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"Erreur OAuth attempt {attempt+1}: {e}\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(\"Impossible d'obtenir un token OAuth après plusieurs essais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ec349-fdb2-4e83-9f7f-da83e1e12a31",
   "metadata": {},
   "source": [
    "### Lancement requête API France Travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e7838-3f2b-4058-95fe-465efefdbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# API CALL FRANCE TRAVAIL\n",
    "# ---------------------------\n",
    "def fetch_france_travail_jobs(query, token, max_pages=MAX_PAGES):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:        \n",
    "        all_jobs = []\n",
    "        b_stop_criteria = False\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if b_stop_criteria == False:    \n",
    "                url = f\"https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search\"\n",
    "                params = {\n",
    "                    \"motsCles\": query,\n",
    "                    \"commune\": COMMUNE,\n",
    "                    \"distance\" : DISTANCE,\n",
    "                    \"range\": f\"{(page-1)*BLOC_PAGINATION}-{page*BLOC_PAGINATION-1}\"  # pagination par blocs de 50\n",
    "                }\n",
    "                r = requests.get(url, headers=headers, params=params)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                offres = data.get(\"resultats\", [])\n",
    "                    \n",
    "                for o in offres:\n",
    "                    all_jobs.append({\n",
    "                        \"origine_annonce\" : \"API\",\n",
    "                        \"source\": \"France Travail\",\n",
    "                        \"recherche\": f\"{query}\",\n",
    "                        \"id\":o.get(\"id\") if o.get(\"id\") is not None else \"None\",    \n",
    "                        \"titre\": o.get(\"intitule\") if o.get(\"intitule\") is not None else \"None\",                     \n",
    "                        \"description\": o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "                        \"entreprise\": o.get(\"entreprise\", {}).get(\"nom\") if o.get(\"entreprise\", {}).get(\"nom\") is not None else \"None\", \n",
    "                        \"lieu\": o.get(\"lieuTravail\", {}).get(\"libelle\") if o.get(\"lieuTravail\", {}).get(\"libelle\") is not None else \"None\", \n",
    "                        \"latitude\": o.get(\"lieuTravail\", {}).get(\"latitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "                        \"longitude\": o.get(\"lieuTravail\", {}).get(\"longitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "                        \"type_contrat_libelle\": o.get(\"typeContratLibelle\") if o.get(\"typeContratLibelle\") is not None else \"None\", \n",
    "                        \"date_publication\": o.get(\"dateCreation\") if o.get(\"dateCreation\") is not None else \"None\",    \n",
    "                        \"url\": o.get(\"origineOffre\").get(\"urlOrigine\") if o.get(\"origineOffre\") is not None else \"None\",\n",
    "                        \"secteur_activites\": o.get(\"secteurActiviteLibelle\") if o.get(\"dateCreation\") is not None else \"None\"\n",
    "                    })\n",
    "    \n",
    "                # Si le nombre d'offres est inférieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres à extraire après la page actuelle.\n",
    "                if len(offres) < BLOC_PAGINATION:\n",
    "                    b_stop_criteria = True\n",
    "                \n",
    "        return all_jobs\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur API France Travail: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f0254-6dfc-4c79-beab-484462e8c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # API CALL FRANCE TRAVAIL\n",
    "# # ---------------------------\n",
    "# def fetch_france_travail_jobs(query, token, max_pages=MAX_PAGES):\n",
    "#     headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "#     try:        \n",
    "#         all_jobs = []\n",
    "#         b_stop_criteria = False\n",
    "        \n",
    "#         for page in range(1, max_pages + 1):\n",
    "#             if b_stop_criteria == False:    \n",
    "#                 url = f\"https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search\"\n",
    "#                 params = {\n",
    "#                     \"motsCles\": query,\n",
    "#                     \"commune\": COMMUNE,\n",
    "#                     \"distance\" : DISTANCE,\n",
    "#                     \"range\": f\"{(page-1)*BLOC_PAGINATION}-{page*BLOC_PAGINATION-1}\"  # pagination par blocs de 50\n",
    "#                 }\n",
    "#                 r = requests.get(url, headers=headers, params=params)\n",
    "#                 r.raise_for_status()\n",
    "#                 data = r.json()\n",
    "#                 offres = data.get(\"resultats\", [])\n",
    "                    \n",
    "#                 for o in offres:\n",
    "#                     all_jobs.append({\n",
    "#                         \"source\": \"France Travail\",\n",
    "#                         \"recherche\": f\"{query}\",\n",
    "#                         \"id\":o.get(\"id\") if o.get(\"id\") is not None else \"None\",    \n",
    "#                         \"titre\": o.get(\"intitule\") if o.get(\"intitule\") is not None else \"None\",                     \n",
    "#                         \"description\": o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "#                         \"entreprise\": o.get(\"entreprise\", {}).get(\"nom\") if o.get(\"entreprise\", {}).get(\"nom\") is not None else \"None\", \n",
    "#                         \"lieu\": o.get(\"lieuTravail\", {}).get(\"libelle\") if o.get(\"lieuTravail\", {}).get(\"libelle\") is not None else \"None\", \n",
    "#                         \"latitude\": o.get(\"lieuTravail\", {}).get(\"latitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "#                         \"longitude\": o.get(\"lieuTravail\", {}).get(\"longitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "#                         \"type_contrat_libelle\": o.get(\"typeContratLibelle\") if o.get(\"typeContratLibelle\") is not None else \"None\", \n",
    "#                         \"date_publication\": o.get(\"dateCreation\") if o.get(\"dateCreation\") is not None else \"None\",    \n",
    "#                         \"url\": o.get(\"origineOffre\").get(\"urlOrigine\") if o.get(\"origineOffre\") is not None else \"None\",\n",
    "#                         \"secteur_activites\": o.get(\"secteurActiviteLibelle\") if o.get(\"dateCreation\") is not None else \"None\"\n",
    "#                     })\n",
    "    \n",
    "#                 # Si le nombre d'offres est inférieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres à extraire après la page actuelle.\n",
    "#                 if len(offres) < BLOC_PAGINATION:\n",
    "#                     b_stop_criteria = True\n",
    "                \n",
    "#         return all_jobs\n",
    "#     except requests.RequestException as e:\n",
    "#         logging.error(f\"Erreur API France Travail: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384c3db-adb2-4c05-96c2-5f75fd670fd9",
   "metadata": {},
   "source": [
    "### Lancement de requête Adzuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86633fe9-b12a-4e55-9a0a-e63ccba9d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# API CALL ADZUNA\n",
    "# ---------------------------\n",
    "def fetch_adzuna_jobs(query, max_pages=MAX_PAGES):\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    try:        \n",
    "        all_jobs = []\n",
    "        b_stop_criteria = False\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if b_stop_criteria == False:    \n",
    "                url = f\"https://api.adzuna.com/v1/api/jobs/fr/search/{page}\"\n",
    "                params = {\n",
    "                    \"app_id\" : ADZUNA_CLIENT_ID,\n",
    "                    \"app_key\" : ADZUNA_CLIENT_SECRET,\n",
    "                    \"title_only\": query,\n",
    "                    \"where\": COMMUNE,\n",
    "                    \"results_per_page\" : BLOC_PAGINATION,\n",
    "                    \"distance\" : DISTANCE\n",
    "                }\n",
    "                r = requests.get(url,params=params)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                offres = data.get(\"results\")\n",
    "    \n",
    "                for o in offres:\n",
    "                    all_jobs.append({\n",
    "                        \"origine_annonce\" : \"API\",\n",
    "                        \"source\": \"Adzuna\",\n",
    "                        \"recherche\":f\"{query}\",\n",
    "                        \"id\" : o.get(\"id\") if o.get(\"id\") is not None else \"None\", \n",
    "                        \"titre\" : o.get(\"title\") if o.get(\"title\") is not None else \"None\", \n",
    "                        \"description\" : o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "                        \"entreprise\": o.get(\"company\").get(\"display_name\") if o.get(\"company\") is not None else \"None\",\n",
    "                        \"lieu\" : o.get(\"location\").get(\"display_name\") if o.get(\"location\") is not None else \"None\",    \n",
    "                        \"latitude\" : o.get(\"latitude\") if o.get(\"latitude\") is not None else \"None\", \n",
    "                        \"longitude\" : o.get(\"longitude\") if o.get(\"longitude\") is not None else \"None\",\n",
    "                        \"type_contrat_libelle\" : o.get(\"contract_type\") if o.get(\"contract_type\") is not None else \"None\",                \n",
    "                        \"date_publication\" : o.get(\"created\") if o.get(\"created\") is not None else \"None\",  \n",
    "                        \"url\" : o.get(\"redirect_url\") if o.get(\"redirect_url\") is not None else \"None\",  \n",
    "                        \"secteur_activites\" : o.get(\"category\").get(\"label\") if o.get(\"category\") is not None else \"None\",\n",
    "                    })\n",
    "                    \n",
    "                # Si le nombre d'offres est inférieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres à extraire après la page actuelle.\n",
    "                if len(offres) < BLOC_PAGINATION:\n",
    "                    b_stop_criteria = True\n",
    "                \n",
    "        return all_jobs\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur API Adzuna: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67e302-f63d-4410-afd3-acc3f277d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # API CALL ADZUNA\n",
    "# # ---------------------------\n",
    "# def fetch_adzuna_jobs(query, max_pages=MAX_PAGES):\n",
    "#     headers = {\"Accept\": \"application/json\"}\n",
    "#     try:        \n",
    "#         all_jobs = []\n",
    "#         b_stop_criteria = False\n",
    "        \n",
    "#         for page in range(1, max_pages + 1):\n",
    "#             if b_stop_criteria == False:    \n",
    "#                 url = f\"https://api.adzuna.com/v1/api/jobs/fr/search/{page}\"\n",
    "#                 params = {\n",
    "#                     \"app_id\" : ADZUNA_CLIENT_ID,\n",
    "#                     \"app_key\" : ADZUNA_CLIENT_SECRET,\n",
    "#                     \"title_only\": query,\n",
    "#                     \"where\": COMMUNE,\n",
    "#                     \"results_per_page\" : BLOC_PAGINATION,\n",
    "#                     \"distance\" : DISTANCE\n",
    "#                 }\n",
    "#                 r = requests.get(url,params=params)\n",
    "#                 r.raise_for_status()\n",
    "#                 data = r.json()\n",
    "#                 offres = data.get(\"results\")\n",
    "    \n",
    "#                 for o in offres:\n",
    "#                     all_jobs.append({\n",
    "#                         \"source\": \"Adzuna\",\n",
    "#                         \"recherche\":f\"{query}\",\n",
    "#                         \"id\" : o.get(\"id\") if o.get(\"id\") is not None else \"None\", \n",
    "#                         \"titre\" : o.get(\"title\") if o.get(\"title\") is not None else \"None\", \n",
    "#                         \"description\" : o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "#                         \"entreprise\": o.get(\"company\").get(\"display_name\") if o.get(\"company\") is not None else \"None\",\n",
    "#                         \"lieu\" : o.get(\"location\").get(\"display_name\") if o.get(\"location\") is not None else \"None\",    \n",
    "#                         \"latitude\" : o.get(\"latitude\") if o.get(\"latitude\") is not None else \"None\", \n",
    "#                         \"longitude\" : o.get(\"longitude\") if o.get(\"longitude\") is not None else \"None\",\n",
    "#                         \"type_contrat_libelle\" : o.get(\"contract_type\") if o.get(\"contract_type\") is not None else \"None\",                \n",
    "#                         \"date_publication\" : o.get(\"created\") if o.get(\"created\") is not None else \"None\",  \n",
    "#                         \"url\" : o.get(\"redirect_url\") if o.get(\"redirect_url\") is not None else \"None\",  \n",
    "#                         \"secteur_activites\" : o.get(\"category\").get(\"label\") if o.get(\"category\") is not None else \"None\",\n",
    "#                     })\n",
    "                    \n",
    "#                 # Si le nombre d'offres est inférieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres à extraire après la page actuelle.\n",
    "#                 if len(offres) < BLOC_PAGINATION:\n",
    "#                     b_stop_criteria = True\n",
    "                \n",
    "#         return all_jobs\n",
    "#     except requests.RequestException as e:\n",
    "#         logging.error(f\"Erreur API Adzuna: {e}\")\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6429abb-111d-4a7a-a1bf-9ad698540ffd",
   "metadata": {},
   "source": [
    "### Déduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da7cdb-9465-4684-957e-041bfe9c26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# DÉDUPLICATION\n",
    "# ---------------------------\n",
    "def deduplicate(jobs):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for job in jobs:\n",
    "        key_str = f\"{job['titre']}_{job['entreprise']}_{job['latitude']}_{job['longitude']}_{job['date_publication']}\"\n",
    "        key = hashlib.md5(key_str.encode()).hexdigest()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(job)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4e04e-f305-4a2a-a890-6fe42ce0437d",
   "metadata": {},
   "source": [
    "### Ajout infos localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be28c71-a049-4af5-84a0-1eb63da04cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# RECHERCHE INFOS LOCALISATION SUPPLEMENTAIRES (commune, code_postal, departement)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def get_localization_info(df):    \n",
    "    PATH_COMMUNES = \"../data/raw_data/location_data/COMMUNE_FRMETDROM.shp\"\n",
    "    \n",
    "    # Extract ccoordonées GPS\n",
    "    coord = df[['longitude','latitude']]\n",
    "    \n",
    "    # Transformer en GeoDataFrame (EPSG:4326 = WGS84 = lat/lon)\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        coord,\n",
    "        geometry=[Point(xy) for xy in zip(coord.longitude, coord.latitude)],\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Charger le shapefile des communes\n",
    "    communes = gpd.read_file(PATH_COMMUNES).to_crs(epsg=4326)\n",
    "\n",
    "    # Jointure spatiale\n",
    "    result = gpd.sjoin(gdf_points, communes, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    # Garder les colonnes utiles\n",
    "    final = result[[\"NOM_M\", \n",
    "                    \"INSEE_COM\", \n",
    "                    \"INSEE_DEP\"]].rename(columns = {\"NOM_M\":\"commune\",\n",
    "                                                    \"INSEE_COM\":\"code_postal\",\n",
    "                                                    \"INSEE_DEP\":\"departement\"})\n",
    "\n",
    "    return pd.merge(df, final, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b7935-0d6b-4bef-9c72-ec09ccc7d2ac",
   "metadata": {},
   "source": [
    "### Nettoyage de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1434de4-10d9-4ba1-b4d1-1dee055bb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e9db3-fa0a-4457-81df-f6bb52f2b2bc",
   "metadata": {},
   "source": [
    "### Calcul embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ae768-d6d1-4cc5-be64-8363f54210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    return model.encode([text], convert_to_numpy=True,show_progress_bar=False)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94663b-d5f9-467c-98c1-0b7cd1c26705",
   "metadata": {},
   "source": [
    "### Initialisation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad547c1c-d524-4cad-8bbf-67afaafddb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialisation DB ---\n",
    "# SUPPRESSION DE Date_creation TIMESTAMP\n",
    "def init_db(engine, table_name):\n",
    "    with engine.begin() as conn:\n",
    "        \n",
    "        # Activer l'extension PGVector\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
    "\n",
    "        # Créer la table\n",
    "        conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            origine_annonce TEXT,\n",
    "            source TEXT,\n",
    "            recherche TEXT,\n",
    "            titre TEXT,\n",
    "            description TEXT,\n",
    "            entreprise TEXT,\n",
    "            lieu TEXT,\n",
    "            latitude FLOAT(4),\n",
    "            longitude FLOAT(4),\n",
    "            commune TEXT,\n",
    "            code_postal TEXT,\n",
    "            departement TEXT,\n",
    "            type_contrat_libelle TEXT,\n",
    "            date_publication DATE,\n",
    "            url TEXT,\n",
    "            secteur_activites TEXT,\n",
    "            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            embedding vector(384),\n",
    "            similitude FLOAT,\n",
    "            candidature_envisagee TEXT,\n",
    "            type_contrat TEXT,\n",
    "            experience_requise TEXT,\n",
    "            candidature_effectuee TEXT,\n",
    "            date_candidature TIMESTAMP,\n",
    "            nom_cv TEXT,\n",
    "            nom_lm TEXT,\n",
    "            nom_fichier_offre TEXT,\n",
    "            date_relance_prevue TIMESTAMP,\n",
    "            date_relance_effectuee TIMESTAMP,\n",
    "            reponse_recue TEXT,\n",
    "            date_reponse_entreprise TIMESTAMP,\n",
    "            etape_atteinte TEXT,\n",
    "            nom_coord_recruteur TEXT,\n",
    "            notes_perso TEXT,\n",
    "            resultat_final TEXT,\n",
    "            nb_jours_candidature_reponse INTEGER,\n",
    "            nb_jours_candidature_resultat_final INTEGER,\n",
    "            score_adequation_poste_profil TEXT,\n",
    "            priorite_offre TEXT,\n",
    "            mots_cles_poste TEXT,\n",
    "            motivation TEXT\n",
    "        )\n",
    "        \"\"\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24a33-1dd6-4d65-9d05-ad203954dbc0",
   "metadata": {},
   "source": [
    "### Sauvegarde en base PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8714f-8ab5-43c6-8e03-98bce597c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres_upsert_initial_api(df, engine, table_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un DataFrame pandas dans PostgreSQL avec UPSERT.\n",
    "    Met à jour last_updated pour chaque ligne insérée ou modifiée.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): données à insérer\n",
    "        engine (sqlalchemy.Engine): moteur SQLAlchemy connecté à PostgreSQL\n",
    "        table_name (str): nom de la table cible\n",
    "    \n",
    "    Returns:\n",
    "        int: nombre de lignes insérées ou mises à jour\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.info(\"📭 DataFrame vide, rien à insérer.\")\n",
    "        return 0\n",
    "\n",
    "    # Nettoyage des NaN\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    now = datetime.utcnow()\n",
    "    count = 0\n",
    "\n",
    "    try:        \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # Calcul embedding uniquement si nouvelle offre\n",
    "                if not row.get(\"embedding\"):\n",
    "                    row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'origine_annonce': stmt.excluded.origine_annonce,\n",
    "                        'source': stmt.excluded.source,\n",
    "                        'recherche':stmt.excluded.recherche,\n",
    "                        'titre': stmt.excluded.titre,\n",
    "                        'description': stmt.excluded.description,\n",
    "                        'entreprise': stmt.excluded.entreprise,\n",
    "                        'lieu': stmt.excluded.lieu,\n",
    "                        'latitude': stmt.excluded.latitude,\n",
    "                        'longitude': stmt.excluded.longitude,   \n",
    "                        'commune': stmt.excluded.commune,   \n",
    "                        'code_postal': stmt.excluded.code_postal,   \n",
    "                        'departement': stmt.excluded.departement,                 \n",
    "                        'type_contrat_libelle': stmt.excluded.type_contrat_libelle,\n",
    "                        'date_publication': stmt.excluded.date_publication,\n",
    "                        'url': stmt.excluded.url,\n",
    "                        'secteur_activites': stmt.excluded.secteur_activites,\n",
    "                        # forcé à chaque exécution, même sans changement d'autres colonnes\n",
    "                        'last_updated': now,\n",
    "                        'embedding': stmt.excluded.embedding\n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        logging.info(f\"{count} offres insérées/mises à jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac263cb7-f4a0-417f-851c-f2690e689095",
   "metadata": {},
   "source": [
    "### Calcul similarité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080956b-6445-497c-abb3-a8e5b660d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_similarity(df, engine, table_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un DataFrame pandas dans PostgreSQL avec UPSERT.\n",
    "    Met à jour last_updated pour chaque ligne insérée ou modifiée.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): données à insérer\n",
    "        engine (sqlalchemy.Engine): moteur SQLAlchemy connecté à PostgreSQL\n",
    "        table_name (str): nom de la table cible\n",
    "    \n",
    "    Returns:\n",
    "        int: nombre de lignes insérées ou mises à jour\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.info(\"📭 DataFrame vide, rien à insérer.\")\n",
    "        return 0\n",
    "\n",
    "    # Nettoyage des NaN\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    now = datetime.utcnow()\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Lancement MAJ du score de similarité dans la base !\")\n",
    "    \n",
    "    try:        \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # Calcul embedding uniquement si nouvelle offre\n",
    "                if not row.get(\"embedding\"):\n",
    "                    row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'similitude': stmt.excluded.similitude, \n",
    "                        # forcé à chaque exécution, même sans changement d'autres colonnes\n",
    "                        'last_updated': now,\n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        logging.info(f\"{count} offres insérées/mises à jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b0410-dbbc-4748-92c2-ced809ec187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(reference_text, engine, table_name):\n",
    "    ref_emb = compute_embedding(reference_text)\n",
    "    ref_emb_str = \"[\" + \",\".join(map(str, ref_emb)) + \"]\"  # convertir en string pour PGVector\n",
    "\n",
    "    query = text(f\"\"\"\n",
    "        SELECT     \n",
    "                id, origine_annonce, source, recherche, titre, description, entreprise, \n",
    "                lieu, latitude, longitude, commune, code_postal, departement, \n",
    "                type_contrat_libelle, date_publication, url, secteur_activites, \n",
    "                last_updated, embedding, similitude, candidature_envisagee, type_contrat, \n",
    "                experience_requise, candidature_effectuee, date_candidature, \n",
    "                nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "                date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "                etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "                nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "                score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "                motivation, simil_temp\n",
    "        FROM (\n",
    "                SELECT \n",
    "                    id, origine_annonce, source, recherche, titre, description, entreprise, \n",
    "                    lieu, latitude, longitude, commune, code_postal, departement, \n",
    "                    type_contrat_libelle, date_publication, url, secteur_activites, \n",
    "                    last_updated, embedding, similitude, candidature_envisagee, type_contrat, \n",
    "                    experience_requise, candidature_effectuee, date_candidature, \n",
    "                    nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "                    date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "                    etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "                    nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "                    score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "                    motivation, 1 - (embedding <#> (:ref)::vector) AS simil_temp\n",
    "                FROM {table_name}\n",
    "            ) AS s\n",
    "        ORDER BY simil_temp DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\"ref\": ref_emb_str})       \n",
    "        df_similarity = pd.DataFrame(result.fetchall(),columns=[\"id\", \"origine_annonce\", \"source\", \"recherche\", \"titre\", \n",
    "                                                                \"description\", \"entreprise\", \"lieu\", \"latitude\", \"longitude\", \n",
    "                                                                \"commune\", \"code_postal\", \"departement\", \"type_contrat_libelle\", \n",
    "                                                                \"date_publication\", \"url\", \"secteur_activites\", \"last_updated\", \n",
    "                                                                \"embedding\", \"similitude\", \"candidature_envisagee\", \"type_contrat\", \n",
    "                                                                \"experience_requise\", \"candidature_effectuee\", \"date_candidature\", \n",
    "                                                                \"nom_cv\", \"nom_lm\", \"nom_fichier_offre\", \"date_relance_prevue\", \n",
    "                                                                \"date_relance_effectuee\", \"reponse_recue\", \"date_reponse_entreprise\", \n",
    "                                                                \"etape_atteinte\", \"nom_coord_recruteur\", \"notes_perso\", \"resultat_final\", \n",
    "                                                                \"nb_jours_candidature_reponse\", \"nb_jours_candidature_resultat_final\", \n",
    "                                                                \"score_adequation_poste_profil\", \"priorite_offre\", \"mots_cles_poste\", \n",
    "                                                                \"motivation\",\"simil_temp\"])       \n",
    "        df_similarity['similitude'] = df_similarity['simil_temp']\n",
    "        df_similarity.drop(columns=['simil_temp'], inplace=True)\n",
    "        update_similarity(df_similarity, engine, table_name)\n",
    "        print(\"Le score de similarité a été mis à jour dans la base !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa781e-c235-48c1-9247-fa1a9a802469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_similarity(reference_text, engine, table_name):\n",
    "#     ref_emb = compute_embedding(reference_text)\n",
    "#     ref_emb_str = \"[\" + \",\".join(map(str, ref_emb)) + \"]\"  # convertir en string pour PGVector\n",
    "\n",
    "#     query = text(f\"\"\"\n",
    "#         SELECT      id, source, recherche, titre, description, entreprise,\n",
    "#                     lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                     type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                     last_updated, embedding,similitude,simil_temp\n",
    "#         FROM (\n",
    "#             SELECT \n",
    "#                     id, source, recherche, titre, description, entreprise,\n",
    "#                     lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                     type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                     last_updated, embedding,similitude,\n",
    "#                     1 - (embedding <#> (:ref)::vector) AS simil_temp\n",
    "#             FROM {table_name}\n",
    "#         ) AS s\n",
    "#         ORDER BY simil_temp DESC\n",
    "#     \"\"\")\n",
    "    \n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(query, {\"ref\": ref_emb_str})       \n",
    "#         df_similarity = pd.DataFrame(result.fetchall(),columns=[\"id\", \"source\", \"recherche\", \"titre\", \"description\", \"entreprise\",\n",
    "#                                                                 \"lieu\", \"latitude\", \"longitude\",\"commune\", \"code_postal\",\"departement\",\n",
    "#                                                                 \"type_contrat_libelle\", \"date_publication\", \"url\", \"secteur_activites\",\n",
    "#                                                                 \"last_updated\", \"embedding\",\"similitude\",\"simil_temp\"])\n",
    "\n",
    "        \n",
    "#         display(df_similarity.head())\n",
    "#         df_similarity['similitude'] = df_similarity['simil_temp']\n",
    "#         display(df_similarity.head())\n",
    "#         df_similarity.drop(columns=['simil_temp'], inplace=True)\n",
    "#         display(df_similarity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c000f-0251-4379-9c06-e9f82071db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_similarity(reference_text, engine):\n",
    "#     ref_emb = compute_embedding(reference_text)\n",
    "#     ref_emb_str = \"[\" + \",\".join(map(str, ref_emb)) + \"]\"  # convertir en string pour PGVector\n",
    "\n",
    "#     query = text(\"\"\"\n",
    "#         SELECT  \n",
    "#                 id, source, recherche, titre, description, entreprise,\n",
    "#                 lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                 type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                 last_updated, embedding, simil\n",
    "#         FROM (\n",
    "#             SELECT \n",
    "#                     id, source, recherche, titre, description, entreprise,\n",
    "#                     lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                     type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                     last_updated, embedding,\n",
    "#                     1 - (embedding <#> (:ref)::vector) AS simil\n",
    "#             FROM offres\n",
    "#         ) AS s\n",
    "#         ORDER BY simil DESC\n",
    "#     \"\"\")\n",
    "    \n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(query, {\"ref\": ref_emb_str})       \n",
    "#         df_similarity = pd.DataFrame(result.fetchall(),columns=[\"id\", \"source\", \"recherche\", \"titre\", \n",
    "#                                                        \"description\", \"entreprise\",\"lieu\",\n",
    "#                                                        \"latitude\", \"longitude\",\"commune\", \n",
    "#                                                        \"code_postal\",\"departement\",\n",
    "#                                                        \"type_contrat_libelle\", \"date_publication\",\n",
    "#                                                        \"url\", \"secteur_activites\",\"last_updated\",\n",
    "#                                                        \"embedding\", \"similitude\"]\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ff738-50ef-4496-9287-57b2d3a7107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_similar_offres_all(reference_text, engine):\n",
    "#     ref_emb = compute_embedding(reference_text)\n",
    "#     ref_emb_str = \"[\" + \",\".join(map(str, ref_emb)) + \"]\"  # convertir en string pour PGVector\n",
    "\n",
    "#     query = text(\"\"\"\n",
    "#         SELECT  \n",
    "#                 id, source, recherche, titre, description, entreprise,\n",
    "#                 lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                 type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                 last_updated, embedding, simil\n",
    "#         FROM (\n",
    "#             SELECT \n",
    "#                     id, source, recherche, titre, description, entreprise,\n",
    "#                     lieu, latitude, longitude,commune, code_postal,departement,\n",
    "#                     type_contrat_libelle, date_publication, url, secteur_activites,\n",
    "#                     last_updated, embedding,\n",
    "#                     1 - (embedding <#> (:ref)::vector) AS simil\n",
    "#             FROM offres\n",
    "#         ) AS s\n",
    "#         ORDER BY simil DESC\n",
    "#     \"\"\")\n",
    "    \n",
    "#     with engine.connect() as conn:\n",
    "#         result = conn.execute(query, {\"ref\": ref_emb_str})       \n",
    "#         return pd.DataFrame(result.fetchall(),columns=[\"id\", \"source\", \"recherche\", \"titre\", \n",
    "#                                                        \"description\", \"entreprise\",\"lieu\",\n",
    "#                                                        \"latitude\", \"longitude\",\"commune\", \n",
    "#                                                        \"code_postal\",\"departement\",\n",
    "#                                                        \"type_contrat_libelle\", \"date_publication\",\n",
    "#                                                        \"url\", \"secteur_activites\",\"last_updated\",\n",
    "#                                                        \"embedding\", \"simil\"]\n",
    "#                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75112a-913c-4cc2-bdfd-f5a320ac2e9d",
   "metadata": {},
   "source": [
    "### Sauvegarde CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0bd1a-552d-4f1a-a5d5-6c71331c54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde Parquet ---\n",
    "def save_to_csv(df, csv_directory, filename):\n",
    "    if df.empty:\n",
    "        return\n",
    "    os.makedirs(csv_directory, exist_ok=True)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    path_csv = os.path.join(csv_directory, f\"{today}_{filename}.csv\")\n",
    "    df.to_csv(path_csv, index=False,encoding=\"utf-8\")\n",
    "    print(f\"✅ Sauvegardé dans {path_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c69b5-e661-49b9-be53-abe6835a3a74",
   "metadata": {},
   "source": [
    "### Sauvegarde Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b972c51-3307-49aa-a409-21b662763b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde Parquet ---\n",
    "def save_to_parquet(df,parquet_directory, filename):\n",
    "    if df.empty:\n",
    "        return\n",
    "    os.makedirs(parquet_directory, exist_ok=True)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    path_parquet = os.path.join(parquet_directory, f\"{today}_{filename}.parquet\")\n",
    "    df.to_parquet(path_parquet, index=False)\n",
    "    print(f\"✅ Sauvegardé dans {path_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14139dd5-4bb0-445d-bd53-56bbb4dfffe0",
   "metadata": {},
   "source": [
    "### Sauvegarde Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f27d3-465d-4937-88cd-decd76bf6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde Excel --- \n",
    "def save_to_excel(df,excel_directory, filename):\n",
    "    if df.empty:\n",
    "        return\n",
    "    os.makedirs(excel_directory, exist_ok=True)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    path_excel = os.path.join(excel_directory, f\"{today}_{filename}.xlsx\")\n",
    "    df.to_excel(path_excel, index=False)\n",
    "    print(f\"✅ Sauvegardé dans {path_excel}\")\n",
    "    return path_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5dcc3-4df3-4bc1-b7f9-8106630cb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_from_table_to_excel(engine, table_name,excel_directory, filename):\n",
    "    query = text(f\"\"\"\n",
    "        SELECT     \n",
    "               id, origine_annonce, source, recherche, titre, \n",
    "               description, entreprise, lieu, latitude, longitude,\n",
    "               commune, code_postal, departement,type_contrat_libelle,\n",
    "               date_publication, url, secteur_activites,last_updated,\n",
    "               embedding, similitude, candidature_envisagee, type_contrat, \n",
    "               experience_requise, candidature_effectuee, date_candidature, \n",
    "               nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "               date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "               etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "               nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "               score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "               motivation\n",
    "        FROM {table_name}\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)       \n",
    "        df_export = pd.DataFrame(result.fetchall(),columns=[\"id\", \"origine_annonce\", \"source\", \"recherche\", \"titre\", \n",
    "                                                                \"description\", \"entreprise\", \"lieu\", \"latitude\", \"longitude\", \n",
    "                                                                \"commune\", \"code_postal\", \"departement\", \"type_contrat_libelle\", \n",
    "                                                                \"date_publication\", \"url\", \"secteur_activites\", \"last_updated\", \n",
    "                                                                \"embedding\", \"similitude\", \"candidature_envisagee\", \"type_contrat\", \n",
    "                                                                \"experience_requise\", \"candidature_effectuee\", \"date_candidature\", \n",
    "                                                                \"nom_cv\", \"nom_lm\", \"nom_fichier_offre\", \"date_relance_prevue\", \n",
    "                                                                \"date_relance_effectuee\", \"reponse_recue\", \"date_reponse_entreprise\", \n",
    "                                                                \"etape_atteinte\", \"nom_coord_recruteur\", \"notes_perso\", \"resultat_final\", \n",
    "                                                                \"nb_jours_candidature_reponse\", \"nb_jours_candidature_resultat_final\", \n",
    "                                                                \"score_adequation_poste_profil\", \"priorite_offre\", \"mots_cles_poste\", \n",
    "                                                                \"motivation\"])       \n",
    "        display(df_export.head())\n",
    "\n",
    "        save_to_excel(df_export,excel_directory, filename)\n",
    "        \n",
    "        print(\"Le score de similarité a été mis à jour dans la base !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef612b-670c-4abc-9fe7-ffc515db1643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250023f3-14a0-4892-97d6-de333098dcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80f5499-2879-4fd3-ba43-9ee1bdeac808",
   "metadata": {},
   "source": [
    "### Mise à jour du fichier de suivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b8736-ad19-4600-ba7b-218a8d31955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracking_file_scrapping(engine, tracking_file_path_dir,filename, table_name):   \n",
    "    try:\n",
    "        # Charger ton fichier de suivi existant\n",
    "        os.makedirs(tracking_file_path_dir, exist_ok=True)\n",
    "        tracking_file_path = os.path.join(tracking_file_path_dir, f\"{filename}.xlsx\")\n",
    "        df = pd.read_excel(tracking_file_path)       \n",
    "\n",
    "        if df.empty:\n",
    "            logging.info(\"📭 DataFrame vide, rien à insérer.\")\n",
    "            return 0\n",
    "    \n",
    "        # Nettoyage des NaN\n",
    "        df = df.where(pd.notnull(df), None)\n",
    "        \n",
    "        metadata = MetaData()\n",
    "        table = Table(table_name, metadata, autoload_with=engine)\n",
    "        now = datetime.utcnow()\n",
    "        count = 0\n",
    "     \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # Calcul embedding uniquement si nouvelle offre\n",
    "                if not row.get(\"embedding\"):\n",
    "                    row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'candidature_envisagee' : stmt.excluded.candidature_envisagee,\n",
    "                        'type_contrat' : stmt.excluded.type_contrat,\n",
    "                        'experience_requise' : stmt.excluded.experience_requise,\n",
    "                        'candidature_effectuee' : stmt.excluded.candidature_effectuee,\n",
    "                        'date_candidature' : stmt.excluded.date_candidature,\n",
    "                        'nom_cv' : stmt.excluded.nom_cv,\n",
    "                        'nom_lm' : stmt.excluded.nom_lm,\n",
    "                        'nom_fichier_offre' : stmt.excluded.nom_fichier_offre,\n",
    "                        'date_relance_prevue' : stmt.excluded.date_relance_prevue,\n",
    "                        'date_relance_effectuee' : stmt.excluded.date_relance_effectuee,\n",
    "                        'reponse_recue' : stmt.excluded.reponse_recue,\n",
    "                        'date_reponse_entreprise' : stmt.excluded.date_reponse_entreprise,\n",
    "                        'etape_atteinte' : stmt.excluded.etape_atteinte,\n",
    "                        'nom_coord_recruteur' : stmt.excluded.nom_coord_recruteur,\n",
    "                        'notes_perso' : stmt.excluded.notes_perso,\n",
    "                        'resultat_final' : stmt.excluded.resultat_final,\n",
    "                        'nb_jours_candidature_reponse' : stmt.excluded.nb_jours_candidature_reponse,\n",
    "                        'nb_jours_candidature_resultat_final' : stmt.excluded.nb_jours_candidature_resultat_final,\n",
    "                        'score_adequation_poste_profil' : stmt.excluded.score_adequation_poste_profil,\n",
    "                        'priorite_offre' : stmt.excluded.priorite_offre,\n",
    "                        'mots_cles_poste' : stmt.excluded.mots_cles_poste,\n",
    "                        'motivation' : stmt.excluded.motivation,\n",
    "                        'last_updated': now       \n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        print(f\"✅ Fichier de suivi mis à jour dans : {output_path_dir}\")\n",
    "        logging.info(f\"{count} offres insérées/mises à jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"❌ Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b53dc-536e-4185-aadd-50fe907bdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_tracking_file_scrapping(engine, tracking_file_path_dir,filename):   \n",
    "#     try:\n",
    "#         # Charger ton fichier de suivi existant\n",
    "#         os.makedirs(tracking_file_path_dir, exist_ok=True)\n",
    "#         tracking_file_path = os.path.join(tracking_file_path_dir, f\"{filename}.xlsx\")\n",
    "#         df = pd.read_excel(tracking_file_path)\n",
    "#     except FileNotFoundError as e:\n",
    "#         logging.exception(f\"Fichier inexistant: {e}\")\n",
    "\n",
    "#     if df.empty:\n",
    "#         logging.info(\"📭 DataFrame vide, rien à insérer.\")\n",
    "#         return 0\n",
    "\n",
    "#     # Nettoyage des NaN\n",
    "#     df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "#     metadata = MetaData()\n",
    "#     table = Table(table_name, metadata, autoload_with=engine)\n",
    "#     now = datetime.utcnow()\n",
    "#     count = 0\n",
    "\n",
    "#     try:        \n",
    "#         with engine.begin() as conn:\n",
    "#             for row in df.to_dict(orient=\"records\"):\n",
    "#                 row[\"last_updated\"] = now\n",
    "                \n",
    "#                 # Calcul embedding uniquement si nouvelle offre\n",
    "#                 if not row.get(\"embedding\"):\n",
    "#                     row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "#                 stmt = pg_insert(table).values(row)\n",
    "#                 stmt = stmt.on_conflict_do_update(\n",
    "#                     index_elements=['id'],\n",
    "#                     set_={\n",
    "#                         'candidature_envisagee' : stmt.excluded.candidature_envisagee,\n",
    "#                         'type_contrat' : stmt.excluded.type_contrat,\n",
    "#                         'experience_requise' : stmt.excluded.experience_requise,\n",
    "#                         'candidature_effectuee' : stmt.excluded.candidature_effectuee,\n",
    "#                         'date_candidature' : stmt.excluded.date_candidature,\n",
    "#                         'nom_cv' : stmt.excluded.nom_cv,\n",
    "#                         'nom_lm' : stmt.excluded.nom_lm,\n",
    "#                         'nom_fichier_offre' : stmt.excluded.nom_fichier_offre,\n",
    "#                         'date_relance_prevue' : stmt.excluded.date_relance_prevue,\n",
    "#                         'date_relance_effectuee' : stmt.excluded.date_relance_effectuee,\n",
    "#                         'reponse_recue' : stmt.excluded.reponse_recue,\n",
    "#                         'date_reponse_entreprise' : stmt.excluded.date_reponse_entreprise,\n",
    "#                         'etape_atteinte' : stmt.excluded.etape_atteinte,\n",
    "#                         'nom_coord_recruteur' : stmt.excluded.nom_coord_recruteur,\n",
    "#                         'notes_perso' : stmt.excluded.notes_perso,\n",
    "#                         'resultat_final' : stmt.excluded.resultat_final,\n",
    "#                         'nb_jours_candidature_reponse' : stmt.excluded.nb_jours_candidature_reponse,\n",
    "#                         'nb_jours_candidature_resultat_final' : stmt.excluded.nb_jours_candidature_resultat_final,\n",
    "#                         'score_adequation_poste_profil' : stmt.excluded.score_adequation_poste_profil,\n",
    "#                         'priorite_offre' : stmt.excluded.priorite_offre,\n",
    "#                         'mots_cles_poste' : stmt.excluded.mots_cles_poste,\n",
    "#                         'motivation' : stmt.excluded.motivation,\n",
    "#                         'last_updated': now       \n",
    "#                     }\n",
    "#                 )\n",
    "#                 conn.execute(stmt)\n",
    "#                 count += 1\n",
    "#         logging.info(f\"{count} offres insérées/mises à jour dans PostgreSQL.\")\n",
    "#         return count\n",
    "\n",
    "#     except SQLAlchemyError as e:\n",
    "#         logging.error(f\"❌ Erreur lors de l'UPSERT : {str(e)}\")\n",
    "#         return 0\n",
    "    \n",
    "#     print(f\"✅ Fichier de suivi mis à jour dans : {output_path_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4fcb2a-470b-43c4-83d8-55dac4fa0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_tracking_file(new_export_path, tracking_file_path_dir, output_path_dir, filename):   \n",
    "#     # tracking_cols = [\"date_candidature\", \"nom_cv\", \"date_relance\", \"reponse_recue\"]\n",
    "#     tracking_cols= [\"candidature_envisagee\",\"type_contrat\",\"experience_requise\",\n",
    "#                     \"date_candidature\", \"date_candidature_jour\", \"date_candidature_mois\", \n",
    "#                     \"date_candidature_annee\", \"nom_cv\", \"nom_lm\", \"nom_fichier_offre\",\n",
    "#                     \"date_relance_prevue\", \"date_relance_effectuee\", \"reponse_recue\",\n",
    "#                     \"date_reponse_entreprise\", \"etape_atteinte\", \"nom_coord_recruteur\",\n",
    "#                     \"notes_perso\", \"resultat_final\", \"nb_jours_candidature_reponse\",\n",
    "#                     \"nb_jours_candidature_resultat_final\",\"score_adequation_poste_profil\",\n",
    "#                     \"priorite_offre\", \"mots_cles_poste\", \"motivation\"]\n",
    "    \n",
    "#     # Charger l’export quotidien depuis PostgreSQL\n",
    "#     new_df = pd.read_excel(new_export_path)\n",
    "\n",
    "#     try:\n",
    "#         # Charger ton fichier de suivi existant\n",
    "#         os.makedirs(tracking_file_path_dir, exist_ok=True)\n",
    "#         tracking_file_path = os.path.join(tracking_file_path_dir, f\"{filename}.xlsx\")\n",
    "#         tracking_df = pd.read_excel(tracking_file_path)\n",
    "#     except FileNotFoundError:\n",
    "#         # Si le fichier n’existe pas encore, on crée le suivi avec les données initiales\n",
    "#         tracking_df = new_df.copy()\n",
    "#         # Ajouter les colonnes manuelles vides\n",
    "#         for col in tracking_cols:\n",
    "#             tracking_df[col] = None\n",
    "\n",
    "#     # Fusionner sur la clé 'id'\n",
    "#     merged_df = pd.merge(\n",
    "#         new_df,\n",
    "#         tracking_df,\n",
    "#         on=\"id\",\n",
    "#         how=\"outer\",\n",
    "#         suffixes=(\"\", \"_old\")\n",
    "#     )\n",
    "    \n",
    "#     # Conserver les colonnes manuelles de l’ancien suivi\n",
    "#     for col in new_df.columns.values.tolist() + tracking_cols:\n",
    "#         if f\"{col}_old\" in merged_df.columns:\n",
    "#             merged_df[col] = merged_df[col].combine_first(merged_df[f\"{col}_old\"])\n",
    "#             merged_df.drop(columns=[f\"{col}_old\"], inplace=True)\n",
    "    \n",
    "#     # Sauvegarder le fichier mis à jour\n",
    "#     save_to_excel(merged_df,output_path_dir, filename)\n",
    "    \n",
    "#     print(f\"✅ Fichier de suivi mis à jour dans : {output_path_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdda02-d6b6-40a2-bfac-934fc24cf646",
   "metadata": {},
   "source": [
    "### Pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6f8148-0086-47ea-b620-5d32c006c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_web_scrapping(query):\n",
    "    logging.info(\"Début du pipeline.\")\n",
    "\n",
    "    try:        \n",
    "        print(\"Authentification France Travail...\")\n",
    "        token = get_ft_token()\n",
    "    \n",
    "        print(\"Récupération des offres France Travail...\")\n",
    "        ft_jobs = fetch_france_travail_jobs(query, token)\n",
    "    \n",
    "        print(\"Récupération des offres Adzuna...\")\n",
    "        adzuna_jobs = fetch_adzuna_jobs(query)\n",
    "    \n",
    "        print(\"Fusion et déduplication...\")\n",
    "        all_jobs = ft_jobs + adzuna_jobs\n",
    "    \n",
    "        if not all_jobs:\n",
    "            print(\"⚠️ Aucune offre trouvée.\")\n",
    "            return\n",
    "    \n",
    "        print(f\"Nombre d'offres d'emploi avant déduplication : {len(all_jobs)}\")\n",
    "        jobs_clean = deduplicate(all_jobs)\n",
    "        print(f\"Nombre d'offres d'emploi après déduplication : {len(jobs_clean)}\")\n",
    "    \n",
    "        print(\"Affichage des offres...\")\n",
    "        df = pd.DataFrame(jobs_clean)\n",
    "\n",
    "        print(\"Ajout commune, code_postal et departement...\")\n",
    "        df = get_localization_info(df)\n",
    "    \n",
    "        # Connexion DB\n",
    "        print(\"Connexion à la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "        # Initier la bdd\n",
    "        api_table_name = 'web_scrapping_table'\n",
    "        init_db(engine, api_table_name)\n",
    "    \n",
    "        print(\"💾 Sauvegarde en base PostgreSQL...\")\n",
    "        save_to_postgres_upsert_initial_api(df, engine, table_name=api_table_name)\n",
    "        print(\"💾 Sauvegarde en base PostgreSQL TERMINEE !!!...\")\n",
    "\n",
    "        print(\"💾 Sauvegarde du score de similarité en base PostgreSQL...\")\n",
    "        compute_similarity(reference_text_clean, engine, api_table_name)\n",
    "        print(\"💾 Sauvegarde du score de similarité en base PostgreSQL TERMINEE !!!...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT DE WEB SCRAPPING !!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline échoué: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed20396-5cbd-428b-a2ae-94b941cc0ad2",
   "metadata": {},
   "source": [
    "### Pipeline EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1090b6f-f40d-496e-9cbe-bf1ff348f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_export_all(table_name):\n",
    "    logging.info(\"Début du pipeline.\")\n",
    "\n",
    "    try:           \n",
    "        # Connexion DB\n",
    "        print(\"Connexion à la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "        # Initier la bdd\n",
    "        init_db(engine, table_name)\n",
    "\n",
    "        print(\"💾 Export de la base initié...\")\n",
    "        export_from_table_to_excel(engine, table_name,EXCEL_DIR_PROC, \"export_base\")\n",
    "        print(\"💾 Export de la base terminé...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT D'EXPORTATION!!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline échoué: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1182481-e737-4f27-8af8-1deb5bf78423",
   "metadata": {},
   "source": [
    "### Pipeline UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee3f22-eb4d-4723-a12c-93b94573e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_update_all(table_name):\n",
    "    logging.info(\"Début du pipeline.\")\n",
    "\n",
    "    try:           \n",
    "        # Connexion DB\n",
    "        print(\"Connexion à la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "        # Initier la bdd\n",
    "        init_db(engine, table_name)\n",
    "\n",
    "        tracking_file_path_dir = EXCEL_DIR_PROC\n",
    "        filename = \"2025-09-21_10-28-10_export_base_MODIF\"\n",
    "\n",
    "        print(\"💾 Update de la base initié...\")\n",
    "        update_tracking_file_scrapping(engine, tracking_file_path_dir,filename,table_name)\n",
    "        print(\"💾 Update de la base terminé...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT D'UPDATE!!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline échoué: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811259b3-4000-47b2-8c99-512f3203cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------- PIPELINE ----------------\n",
    "# def run_pipeline(query):\n",
    "#     logging.info(\"Début du pipeline.\")\n",
    "\n",
    "#     try:        \n",
    "#         print(\"Authentification France Travail...\")\n",
    "#         token = get_ft_token()\n",
    "    \n",
    "#         print(\"Récupération des offres France Travail...\")\n",
    "#         ft_jobs = fetch_france_travail_jobs(query, token)\n",
    "    \n",
    "#         print(\"Récupération des offres Adzuna...\")\n",
    "#         adzuna_jobs = fetch_adzuna_jobs(query)\n",
    "    \n",
    "#         print(\"Fusion et déduplication...\")\n",
    "#         all_jobs = ft_jobs + adzuna_jobs\n",
    "    \n",
    "#         if not all_jobs:\n",
    "#             print(\"⚠️ Aucune offre trouvée.\")\n",
    "#             return\n",
    "    \n",
    "#         print(f\"Nombre d'offres d'emploi avant déduplication : {len(all_jobs)}\")\n",
    "#         jobs_clean = deduplicate(all_jobs)\n",
    "#         print(f\"Nombre d'offres d'emploi après déduplication : {len(jobs_clean)}\")\n",
    "    \n",
    "#         print(\"Affichage des offres...\")\n",
    "#         df = pd.DataFrame(jobs_clean)\n",
    "\n",
    "#         print(\"Ajout commune, code_postal et departement...\")\n",
    "#         df = get_localization_info(df)\n",
    "        \n",
    "#         # # Export vers CSV\n",
    "#         # print(\"💾 Sauvegarde en CSV...\")\n",
    "#         # save_to_csv(df, CSV_DIR_RAW,'offres_brutes')\n",
    "    \n",
    "#         # # Export vers Parquet\n",
    "#         # print(\"💾 Sauvegarde en Parquet...\")\n",
    "#         # save_to_parquet(df, PARQUET_DIR_RAW,'offres_brutes')\n",
    "        \n",
    "#         print(f\"{len(jobs_clean)} offres uniques exportées dans {CSV_DIR_RAW} et {PARQUET_DIR_RAW} ✅\")\n",
    "    \n",
    "#         # Connexion DB\n",
    "#         print(\"Connexion à la base PostgreSQL...\")\n",
    "#         engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "#         # Initier la bdd\n",
    "#         init_db(engine)\n",
    "    \n",
    "#         print(\"💾 Sauvegarde en base PostgreSQL...\")\n",
    "#         save_to_postgres_upsert(df, engine, table_name=DB_TABLE_NAME)\n",
    "#         print(\"💾 Sauvegarde en base PostgreSQL TERMINEE !!!...\")\n",
    "\n",
    "#         # Recherche top offres similaires\n",
    "#         print(\"Lancement requête en similarité ...\")\n",
    "#         top_offres = search_similar_offres_all(reference_text_clean,engine)\n",
    "#         logging.info(f\"Top 10 offres les plus proches:\\n{top_offres}\")\n",
    "#         logging.info(\"Pipeline terminé avec succès.\")\n",
    "\n",
    "#         # # Export vers CSV\n",
    "#         # print(\"💾 Sauvegarde en CSV (avec Similarité)...\")\n",
    "#         # save_to_csv(top_offres, CSV_DIR_PROC,'offres_similarite')\n",
    "    \n",
    "#         # # Export vers Parquet\n",
    "#         # print(\"💾 Sauvegarde en Parquet (avec Similarité)...\")\n",
    "#         # save_to_parquet(top_offres, PARQUET_DIR_PROC,'offres_similarite')\n",
    "\n",
    "#         # Export vers Excel\n",
    "#         print(\"💾 Sauvegarde en Excel (avec Similarité)...\")\n",
    "#         similarity_excel_path = save_to_excel(top_offres, EXCEL_DIR_PROC,'offres_similarite')\n",
    "\n",
    "#         # Mise à jour du fichier de suivi vers Excel\n",
    "#         print(\"Mise à jour du fichier de suivi...\")\n",
    "#         tracking_filename = \"suivi_candidatures\"\n",
    "#         tracking_file_path_dir=f\"../data/processed_data/suivi_candidature/input_tracking_file\"\n",
    "#         output_path_dir=f\"../data/processed_data/suivi_candidature/output_tracking_file\"        \n",
    "#         update_tracking_file(\n",
    "#                                 similarity_excel_path,\n",
    "#                                 tracking_file_path_dir,\n",
    "#                                 output_path_dir,\n",
    "#                                 tracking_filename,\n",
    "#                             )\n",
    "    \n",
    "#         # Affichage extract offres\n",
    "#         display(top_offres.shape)\n",
    "#         display(top_offres.head())\n",
    "\n",
    "#         # # Affichage extract offres\n",
    "#         # display(df.shape)\n",
    "#         # display(df.head(3))\n",
    "\n",
    "#         print(\"FIN DU SCRIPT !!!...\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         logging.exception(f\"Pipeline échoué: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de560b5-c04e-4118-9662-7069e1da4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chargement depuis CSV\n",
    "# today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# path_parquet = os.path.join(PARQUET_DIR, f\"{today}_offres.parquet\")\n",
    "# df = pd.read_parquet(path_parquet)\n",
    "# display(df.shape)\n",
    "# display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859727e-da41-42a1-9772-fa2708d9d558",
   "metadata": {},
   "source": [
    "### Procédure principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff03fb25-913a-46ea-a784-b7f5f54e9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    JOB_QUERY = [\"data analyst\"]\n",
    "    api_table_name = 'web_scrapping_table'\n",
    "    for query in JOB_QUERY:\n",
    "        # run_pipeline_web_scrapping(query)\n",
    "        run_pipeline_export_all(api_table_name)\n",
    "        # run_pipeline_update_all(api_table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
