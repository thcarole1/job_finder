{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598d621d-ffac-4fe6-8e18-07c055e9d366",
   "metadata": {},
   "source": [
    "# Connexion_France_Travail_ADZUNA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e368b-be9e-4195-b2ab-5f3eebcdcc12",
   "metadata": {},
   "source": [
    "Ce script a pour objectif :\n",
    "- d'extraire les offres d'emploi mises √† disposition par :\n",
    "  <br> _ l'API de **France Travail**\n",
    "  <br>_ de l'API **ADZUNA**\n",
    "- les stocker dans :\n",
    "  <br> _ un fichier (**CSV**) en local\n",
    "  <br> _ un fichier (**Parquet**) en local\n",
    "  <br> _ dans une **BDD PostgreSQL** en local.\n",
    "\n",
    "**/!\\ Ajouts !** : \n",
    "- Voir ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ec277-2fae-46cc-bbe0-839e4322c3be",
   "metadata": {},
   "source": [
    "1.\tAjout d‚Äôune table d√©di√©e √† la r√©cup√©ration d‚Äôoffres d‚Äôemploi via API\n",
    "- Nom de la nouvelle table : ‚Äòweb_scrapping_table‚Äô\n",
    "- Modification du sch√©ma g√©n√©ral de la table avec :\n",
    "  <br>_Ajout d‚Äôun champ indiquant l‚Äôorigine des donn√©es : ‚Äòorigine_annonce‚Äô, ‚Äòcandidature_effectuee‚Äô\n",
    "  <br>_Suppression des champs ‚Äòdate_candidature_jour‚Äô, ‚Äòdate_candidature_mois‚Äô et ‚Äòdate_candidature_annee‚Äô\n",
    "- Modification de la fonction init_db(engine, table_name) : OK !!!\n",
    "\n",
    "2.\tIns√©rer les offres via api dans la table d√©di√©e aux offres API\n",
    "- Ne pas remplir les colonnes ‚Äòmanuelles‚Äô pour le suivi.\n",
    "- Remplir le champ ‚Äòorigine_annonce‚Äô avec la valeur ‚ÄòAPI‚Äô\n",
    "- Calculer les embeddings\n",
    "- Cr√©ation de la fonction ‚Äòsave_to_postgres_upsert_initial_api‚Äô : OK !!\n",
    "\n",
    "3.\tCr√©er une fonction g√©n√©rique pour calculer le score de similarit√©. Cette fonction peut √™tre appel√©e sur une table quelconque\n",
    "- Une fonction ‚Äúcompute_similarity(reference_text, engine, table_name)‚Äù calcule dans un premier temps la similitude entre texte de r√©f√©rence et description de chaque offre.\n",
    "- Une fonction ¬´ update_similarity ¬ª int√®gre ce score de similitude dans la table d√©di√©e de la base de donn√©es.\n",
    "- Cr√©ation OK !!\n",
    "\n",
    "4.\tCr√©er une fonction g√©n√©rique d‚Äôexport d‚Äôune table de la base de donn√©es\n",
    "- Cr√©ation OK !!\n",
    "\n",
    "5.\tCr√©er une fonction de mise √† jour de la table d√©di√©e API √† partir du fichier de suivi\n",
    "- Cr√©ation OK !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7129f84d-2d01-44eb-ba17-25d4cf673d3c",
   "metadata": {},
   "source": [
    "Comment ?\n",
    "1. Sur la base de crit√®res sp√©cifiques (mots cl√©s, localisation, etc...), \n",
    "    - lancement d'une requ√™te pour obtenir les offres d'emploi correspondantes via l'API France Travail\n",
    "    - lancement d'une requ√™te pour obtenir les offres d'emploi correspondantes via l'API Adzuna\n",
    "2. Une fois les offres trouv√©es, v√©rification et suppression des doublons.\n",
    "3. Une sauvegarde en local des offres sont stock√©es dans un fichier (**CSV**).\n",
    "4. Une sauvegarde en local des offres sont stock√©es dans un fichier (**Parquet**).\n",
    "5. Une sauvegarde dans une base de donn√©es **PostgreSQL** est √©galement effectu√©e en local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6874620-7676-4138-9b30-0ac53198d8f1",
   "metadata": {},
   "source": [
    "Les URL (FRANCE TRAVAIL) utiles sont :\n",
    "- https://francetravail.io/data/api/offres-emploi\n",
    "- https://francetravail.io/data/api/offres-emploi/documentation#/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736e6c8-5d60-47e3-8eb8-cbf900de62c1",
   "metadata": {},
   "source": [
    "Les URL (ADZUNA) utiles sont :\n",
    "- https://developer.adzuna.com/overview\n",
    "- https://developer.adzuna.com/docs/search\n",
    "- https://developer.adzuna.com/activedocs#!/adzuna/search\n",
    "- https://developer.adzuna.com/overview\n",
    "- https://www.adzuna.fr/details/5376850320?utm_medium=api&utm_source=6d1ef246"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b6efe-f320-4b0d-86ec-44109e1620b6",
   "metadata": {},
   "source": [
    "URL utile pour r√©cup√©rer les informations g√©ographiques:\n",
    "- https://www.data.gouv.fr/datasets/contours-communes-france-administrative-format-admin-express-avec-arrondissements/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77453b-9814-4567-97aa-7f680bfdfe9b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "id": "59e94172-c17e-4a12-9519-d1b26cd600df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, Table, MetaData, text\n",
    "from sqlalchemy.dialects.postgresql import insert as pg_insert\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import http.client\n",
    "import json\n",
    "import hashlib\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dd34f-6c24-4f94-b985-40336f97d531",
   "metadata": {},
   "source": [
    "## Proc√©dure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d771d39-0166-45aa-abda-90d58d82f440",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "id": "17e18cf1-1961-4200-9bb9-497ce803db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################  VARIABLES  ##################\n",
    "# France Travail\n",
    "FT_CLIENT_ID = os.environ.get(\"FT_CLIENT_ID\")\n",
    "FT_CLIENT_SECRET = os.environ.get(\"FT_CLIENT_SECRET\")\n",
    "FT_SCOPE = os.environ.get(\"FT_SCOPE\")\n",
    "\n",
    "# Adzuna\n",
    "ADZUNA_CLIENT_ID = os.environ.get(\"ADZUNA_CLIENT_ID\")\n",
    "ADZUNA_CLIENT_SECRET = os.environ.get(\"ADZUNA_CLIENT_SECRET\")\n",
    "\n",
    "# Param Database PostgreSQL\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"jobsdb\")\n",
    "DB_USER = os.environ.get(\"DB_USER\",\"jobsuser\")\n",
    "DB_PASS = os.environ.get(\"DB_PASS\", \"jobspass\")\n",
    "DB_HOST = os.environ.get(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\",\"5432\")\n",
    "\n",
    "# Nom table\n",
    "DB_TABLE_NAME = \"offres\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f0bec-9475-4dd1-92d6-f6a21a487f7e",
   "metadata": {},
   "source": [
    "### Configuration Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "id": "aa3f970c-1849-454a-8191-ebe47ac0008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "LOG_DIR = \"logs\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(LOG_DIR, f\"pipeline_{datetime.now().strftime('%Y-%m-%d')}.log\"),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7dbbdb-2ef2-494a-ad27-31bb79f36ba1",
   "metadata": {},
   "source": [
    "### NLP ET embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "id": "6f627bbe-919e-48bf-917d-0f50965505a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP & embeddings\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c6b29-9af0-4439-a594-b5527a2dee1d",
   "metadata": {},
   "source": [
    "### Texte de r√©f√©rence (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "id": "18f34d4e-8de1-45c8-a307-edcd2505f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offre de r√©f√©rence\n",
    "reference_text = \"\"\"\n",
    "-\tData Analyst\n",
    "-\tData Scientist\n",
    "-\tData Analyst en reconversion\n",
    "-\t10 ans d‚Äôexp√©rience industrie automobile & achats\n",
    "-\tExpert en dashboards et optimisation de performance\n",
    "\n",
    "-\tLocalisation :\n",
    "o\tIle-de-France\n",
    "o\tYvelines\n",
    "o\tPoissy\n",
    "o\t78\n",
    "\n",
    "-\tComp√©tences :\n",
    "o\tData Analysis & BI : Power BI (DAX, Power Query), Excel avanc√©, SQL.\n",
    "o\tData Visualization : Cr√©ation de tableaux de bord et KPIs pour la prise de d√©cision.\n",
    "o\tStatistiques & Pr√©visions : Analyses quantitatives, mod√®les pr√©dictifs, pr√©ventions des risques.\n",
    "o\tConception d‚Äôoutils d‚Äôaide √† la d√©cision et de tableaux de bord strat√©giques\n",
    "o\tM√©thodologies : Gestion de projets analytiques, reporting automatis√©.\n",
    "o\tExploitation de solutions de Data Science & Intelligence Artificielle : Machine Learning, mod√®les pr√©dictifs, classification, r√©gression, clustering\n",
    "o\tAnalyses et mod√©lisations statistiques avanc√©es : Python, outils BI, Dataiku\n",
    "o\tGestion et structuration de donn√©es massives : SQL Server, MySQL, Cloud Azure (Machine Learning Studio).\n",
    "o\tD√©veloppement et optimisation d‚Äôalgorithmes : pour la performance et l‚Äôautomatisation.\n",
    "o\tConception d‚Äôoutils d‚Äôaide √† la d√©cision et de tableaux de bord strat√©giques pour orienter les choix business.\n",
    "\n",
    "-\tInformatique :\n",
    "o\tLangages de programmation : Python, Java, SQL\n",
    "o\tLogiciel : Power BI, Excel, Dataiku, Jupyter Notebook\n",
    "o\tCloud : Azure, Google Cloud Platform\n",
    "\n",
    "-\tDipl√¥mes et Formations :\n",
    "o\tCertification Microsoft Analyste de Donn√©es Power BI (PL300)\n",
    "o\tBac+4 - Concepteur d√©veloppeur en IA et analyse Big Data\n",
    "o\tBac+5 ‚ÄìMaster 2 Electronique Electrotechnique et Automatique\n",
    "\n",
    "-\tAtouts :\n",
    "o\tAutonome et rigoureux dans la gestion de projets.\n",
    "o\tVulgarise des r√©sultats complexes pour des non-sp√©cialistes.\n",
    "o\tOrganise et priorise les t√¢ches orient√©es r√©sultats.\n",
    "o\tEn veille active sur l‚ÄôIA et les nouvelles technologies.\n",
    "o\tCurieux\n",
    "o\tAutonome et rigoureux\n",
    "o\tForce de proposition\n",
    "o\tA l'√©coute\n",
    "o\tEsprit d'√©quipe\n",
    "-\tLangues : Anglais courant\n",
    "-\tExp√©riences professionnelles : \n",
    "o\tAcheteur de composants : \n",
    "ÔÇß\tAnalyser et structurer des donn√©es fournisseurs pour l‚Äôoptimisation des co√ªts.\n",
    "ÔÇß\tD√©velopper des tableaux de bord (KPI, suivi de performance) automatis√©s.\n",
    "ÔÇß\tCommuniquer des insights aux √©quipes finance, qualit√© et production.\n",
    "ÔÇß\tR√©aliser des √©conomies sup√©rieures de 20 % aux objectifs fix√©s.\n",
    "ÔÇß\tAnalyser et structurer des donn√©es massives pour optimiser les co√ªts et la performance fournisseurs.\n",
    "ÔÇß\tCr√©er et automatiser de tableaux de bord (Power BI, Excel avanc√©) pour le suivi des KPIs.\n",
    "ÔÇß\tD√©velopper des mod√®les pr√©dictifs pour la pr√©vision des co√ªts et l‚Äôanalyse de tendances.\n",
    "ÔÇß\tG√©rer des projets interfonctionnels (production, finance, qualit√©), g√©n√©rant +20 % d‚Äô√©conomies au-del√† des objectifs.\n",
    "o\tResponsable de d√©veloppement de machines √©lectrique :\n",
    "ÔÇß\tAnalyser et valider des donn√©es issues des tests de performance produits.\n",
    "ÔÇß\tAutomatiser des traitements statistiques pour r√©duire les erreurs de reporting.\n",
    "ÔÇß\tMettre en place de mod√®les pr√©dictifs pour am√©liorer la fiabilit√© des composants.\n",
    "ÔÇß\tConcevoir des solutions analytiques pour optimiser la durabilit√© et la performance des composants.\n",
    "ÔÇß\tCollaborer en mode Agile avec √©quipes R&D et Data pour int√©grer l‚Äôanalyse dans l‚Äôam√©lioration continue.\n",
    "ÔÇß\tAnalyse statistique et validation de donn√©es issues de tests de performance.\n",
    "-\tCentres d‚Äôint√©r√™t :\n",
    "o\tTh√©√¢tre : Improvisation\n",
    "o\tMoto : Sorties en groupe\n",
    "\"\"\"\n",
    "reference_text_clean = \" \".join([token.lemma_ for token in nlp(reference_text.lower()) if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631ac5b-b706-486a-bea2-e47ef6a4f9a4",
   "metadata": {},
   "source": [
    "### Param√®tres de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "id": "8c47f4fc-a8ce-482c-ae6d-250061c5682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# PARAMETRES DE RECHERCHE\n",
    "# ---------------------------\n",
    "# Param√®tres de recherche\n",
    "# JOB_QUERY = \"data analyst\"\n",
    "COMMUNE = \"78300\"\n",
    "DISTANCE = 100000\n",
    "\n",
    "# Nombre d'annonces par page requise\n",
    "BLOC_PAGINATION = 50\n",
    "\n",
    "# Nombre de pages max\n",
    "MAX_PAGES = 20   # Limiter le nombre de pages r√©cup√©r√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c2baf-0c81-4879-9cbb-7d3cad0f33a0",
   "metadata": {},
   "source": [
    "### Param√®tres de sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "id": "13daef8d-56fc-4538-994e-cecd5e78e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# PARAMETRES DE SAUVEGARDE\n",
    "# ---------------------------\n",
    "# R√©pertoires\n",
    "# Processed_data\n",
    "IMPORT_TRACKING_DIR_PROC = \"../data/processed_data/suivi_candidature/input_tracking_file\"\n",
    "EXPORT_TRACKING_DIR_PROC = \"../data/processed_data/suivi_candidature/output_tracking_file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b50db3-037e-4050-b095-393fd8ca5aa1",
   "metadata": {},
   "source": [
    "### Authentification France Travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "id": "af447536-bcf7-4def-8b46-a45c510ff850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# AUTH FRANCE TRAVAIL\n",
    "# ---------------------------\n",
    "def get_ft_token(retries=3, wait=5):\n",
    "    url = \"https://entreprise.pole-emploi.fr/connexion/oauth2/access_token?realm=/partenaire\"\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"client_id\": FT_CLIENT_ID,\n",
    "        \"client_secret\": FT_CLIENT_SECRET,\n",
    "        \"scope\": FT_SCOPE,\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            r = requests.post(url, data=data)\n",
    "            r.raise_for_status()\n",
    "            return r.json()[\"access_token\"]\n",
    "        except requests.RequestException as e:\n",
    "            logging.warning(f\"Erreur OAuth attempt {attempt+1}: {e}\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(\"Impossible d'obtenir un token OAuth apr√®s plusieurs essais.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ec349-fdb2-4e83-9f7f-da83e1e12a31",
   "metadata": {},
   "source": [
    "### Lancement requ√™te API France Travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "id": "e82e7838-3f2b-4058-95fe-465efefdbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# API CALL FRANCE TRAVAIL\n",
    "# ---------------------------\n",
    "def fetch_france_travail_jobs(query, token, max_pages=MAX_PAGES):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    try:        \n",
    "        all_jobs = []\n",
    "        b_stop_criteria = False\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if b_stop_criteria == False:    \n",
    "                url = f\"https://api.francetravail.io/partenaire/offresdemploi/v2/offres/search\"\n",
    "                params = {\n",
    "                    \"motsCles\": query,\n",
    "                    \"commune\": COMMUNE,\n",
    "                    \"distance\" : DISTANCE,\n",
    "                    \"range\": f\"{(page-1)*BLOC_PAGINATION}-{page*BLOC_PAGINATION-1}\"  # pagination par blocs de 50\n",
    "                }\n",
    "                r = requests.get(url, headers=headers, params=params)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                offres = data.get(\"resultats\", [])\n",
    "                    \n",
    "                for o in offres:\n",
    "                    all_jobs.append({\n",
    "                        \"origine_annonce\" : \"API\",\n",
    "                        \"source\": \"France Travail\",\n",
    "                        \"recherche\": f\"{query}\",\n",
    "                        \"id\":o.get(\"id\") if o.get(\"id\") is not None else \"None\",    \n",
    "                        \"titre\": o.get(\"intitule\") if o.get(\"intitule\") is not None else \"None\",                     \n",
    "                        \"description\": o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "                        \"entreprise\": o.get(\"entreprise\", {}).get(\"nom\") if o.get(\"entreprise\", {}).get(\"nom\") is not None else \"None\", \n",
    "                        \"lieu\": o.get(\"lieuTravail\", {}).get(\"libelle\") if o.get(\"lieuTravail\", {}).get(\"libelle\") is not None else \"None\", \n",
    "                        \"latitude\": o.get(\"lieuTravail\", {}).get(\"latitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "                        \"longitude\": o.get(\"lieuTravail\", {}).get(\"longitude\") if o.get(\"lieuTravail\", {}) is not None else \"None\", \n",
    "                        \"type_contrat_libelle\": o.get(\"typeContratLibelle\") if o.get(\"typeContratLibelle\") is not None else \"None\", \n",
    "                        \"date_publication\": o.get(\"dateCreation\") if o.get(\"dateCreation\") is not None else \"None\",    \n",
    "                        \"url\": o.get(\"origineOffre\").get(\"urlOrigine\") if o.get(\"origineOffre\") is not None else \"None\",\n",
    "                        \"secteur_activites\": o.get(\"secteurActiviteLibelle\") if o.get(\"dateCreation\") is not None else \"None\"\n",
    "                    })\n",
    "    \n",
    "                # Si le nombre d'offres est inf√©rieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres √† extraire apr√®s la page actuelle.\n",
    "                if len(offres) < BLOC_PAGINATION:\n",
    "                    b_stop_criteria = True\n",
    "                \n",
    "        return all_jobs\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur API France Travail: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c384c3db-adb2-4c05-96c2-5f75fd670fd9",
   "metadata": {},
   "source": [
    "### Lancement de requ√™te Adzuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "id": "86633fe9-b12a-4e55-9a0a-e63ccba9d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# API CALL ADZUNA\n",
    "# ---------------------------\n",
    "def fetch_adzuna_jobs(query, max_pages=MAX_PAGES):\n",
    "    headers = {\"Accept\": \"application/json\"}\n",
    "    try:        \n",
    "        all_jobs = []\n",
    "        b_stop_criteria = False\n",
    "        \n",
    "        for page in range(1, max_pages + 1):\n",
    "            if b_stop_criteria == False:    \n",
    "                url = f\"https://api.adzuna.com/v1/api/jobs/fr/search/{page}\"\n",
    "                params = {\n",
    "                    \"app_id\" : ADZUNA_CLIENT_ID,\n",
    "                    \"app_key\" : ADZUNA_CLIENT_SECRET,\n",
    "                    \"title_only\": query,\n",
    "                    \"where\": COMMUNE,\n",
    "                    \"results_per_page\" : BLOC_PAGINATION,\n",
    "                    \"distance\" : DISTANCE\n",
    "                }\n",
    "                r = requests.get(url,params=params)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                offres = data.get(\"results\")\n",
    "    \n",
    "                for o in offres:\n",
    "                    all_jobs.append({\n",
    "                        \"origine_annonce\" : \"API\",\n",
    "                        \"source\": \"Adzuna\",\n",
    "                        \"recherche\":f\"{query}\",\n",
    "                        \"id\" : o.get(\"id\") if o.get(\"id\") is not None else \"None\", \n",
    "                        \"titre\" : o.get(\"title\") if o.get(\"title\") is not None else \"None\", \n",
    "                        \"description\" : o.get(\"description\") if o.get(\"description\") is not None else \"None\", \n",
    "                        \"entreprise\": o.get(\"company\").get(\"display_name\") if o.get(\"company\") is not None else \"None\",\n",
    "                        \"lieu\" : o.get(\"location\").get(\"display_name\") if o.get(\"location\") is not None else \"None\",    \n",
    "                        \"latitude\" : o.get(\"latitude\") if o.get(\"latitude\") is not None else \"None\", \n",
    "                        \"longitude\" : o.get(\"longitude\") if o.get(\"longitude\") is not None else \"None\",\n",
    "                        \"type_contrat_libelle\" : o.get(\"contract_type\") if o.get(\"contract_type\") is not None else \"None\",                \n",
    "                        \"date_publication\" : o.get(\"created\") if o.get(\"created\") is not None else \"None\",  \n",
    "                        \"url\" : o.get(\"redirect_url\") if o.get(\"redirect_url\") is not None else \"None\",  \n",
    "                        \"secteur_activites\" : o.get(\"category\").get(\"label\") if o.get(\"category\") is not None else \"None\",\n",
    "                    })\n",
    "                    \n",
    "                # Si le nombre d'offres est inf√©rieur au nombre max d'offre par pages, c'est un signe qu'il n'y a plus d'offres √† extraire apr√®s la page actuelle.\n",
    "                if len(offres) < BLOC_PAGINATION:\n",
    "                    b_stop_criteria = True\n",
    "                \n",
    "        return all_jobs\n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Erreur API Adzuna: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6429abb-111d-4a7a-a1bf-9ad698540ffd",
   "metadata": {},
   "source": [
    "### D√©duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "id": "98da7cdb-9465-4684-957e-041bfe9c26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# D√âDUPLICATION\n",
    "# ---------------------------\n",
    "def deduplicate(jobs):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for job in jobs:\n",
    "        key_str = f\"{job['titre']}_{job['entreprise']}_{job['latitude']}_{job['longitude']}_{job['date_publication']}\"\n",
    "        key = hashlib.md5(key_str.encode()).hexdigest()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(job)\n",
    "    return deduped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4e04e-f305-4a2a-a890-6fe42ce0437d",
   "metadata": {},
   "source": [
    "### Ajout infos localisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "id": "9be28c71-a049-4af5-84a0-1eb63da04cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# RECHERCHE INFOS LOCALISATION SUPPLEMENTAIRES (commune, code_postal, departement)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def get_localization_info(df):    \n",
    "    PATH_COMMUNES = \"../data/raw_data/location_data/COMMUNE_FRMETDROM.shp\"\n",
    "    \n",
    "    # Extract ccoordon√©es GPS\n",
    "    coord = df[['longitude','latitude']]\n",
    "    \n",
    "    # Transformer en GeoDataFrame (EPSG:4326 = WGS84 = lat/lon)\n",
    "    gdf_points = gpd.GeoDataFrame(\n",
    "        coord,\n",
    "        geometry=[Point(xy) for xy in zip(coord.longitude, coord.latitude)],\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "\n",
    "    # Charger le shapefile des communes\n",
    "    communes = gpd.read_file(PATH_COMMUNES).to_crs(epsg=4326)\n",
    "\n",
    "    # Jointure spatiale\n",
    "    result = gpd.sjoin(gdf_points, communes, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    # Garder les colonnes utiles\n",
    "    final = result[[\"NOM_M\", \n",
    "                    \"INSEE_COM\", \n",
    "                    \"INSEE_DEP\"]].rename(columns = {\"NOM_M\":\"commune\",\n",
    "                                                    \"INSEE_COM\":\"code_postal\",\n",
    "                                                    \"INSEE_DEP\":\"departement\"})\n",
    "\n",
    "    return pd.merge(df, final, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b7935-0d6b-4bef-9c72-ec09ccc7d2ac",
   "metadata": {},
   "source": [
    "### Nettoyage de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "id": "d1434de4-10d9-4ba1-b4d1-1dee055bb332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    doc = nlp(text.lower())\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80e9db3-fa0a-4457-81df-f6bb52f2b2bc",
   "metadata": {},
   "source": [
    "### Calcul embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "id": "f85ae768-d6d1-4cc5-be64-8363f54210ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding(text):\n",
    "    return model.encode([text], convert_to_numpy=True,show_progress_bar=False)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94663b-d5f9-467c-98c1-0b7cd1c26705",
   "metadata": {},
   "source": [
    "### Initialisation database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "id": "ad547c1c-d524-4cad-8bbf-67afaafddb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialisation DB ---\n",
    "# SUPPRESSION DE Date_creation TIMESTAMP\n",
    "def init_db(engine, table_name):\n",
    "    with engine.begin() as conn:\n",
    "        \n",
    "        # Activer l'extension PGVector\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
    "\n",
    "        # Cr√©er la table\n",
    "        conn.execute(text(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            origine_annonce TEXT,\n",
    "            source TEXT,\n",
    "            recherche TEXT,\n",
    "            titre TEXT,\n",
    "            description TEXT,\n",
    "            entreprise TEXT,\n",
    "            lieu TEXT,\n",
    "            latitude FLOAT(4),\n",
    "            longitude FLOAT(4),\n",
    "            commune TEXT,\n",
    "            code_postal TEXT,\n",
    "            departement TEXT,\n",
    "            type_contrat_libelle TEXT,\n",
    "            date_publication TIMESTAMP,\n",
    "            url TEXT,\n",
    "            secteur_activites TEXT,\n",
    "            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            embedding vector(384),\n",
    "            similitude FLOAT,\n",
    "            candidature_envisagee TEXT,\n",
    "            type_contrat TEXT,\n",
    "            experience_requise TEXT,\n",
    "            candidature_effectuee TEXT,\n",
    "            date_candidature DATE,\n",
    "            nom_cv TEXT,\n",
    "            nom_lm TEXT,\n",
    "            nom_fichier_offre TEXT,\n",
    "            date_relance_prevue DATE,\n",
    "            date_relance_effectuee DATE,\n",
    "            reponse_recue TEXT,\n",
    "            date_reponse_entreprise DATE,\n",
    "            etape_atteinte TEXT,\n",
    "            nom_coord_recruteur TEXT,\n",
    "            notes_perso TEXT,\n",
    "            resultat_final TEXT,\n",
    "            nb_jours_candidature_reponse INTEGER,\n",
    "            nb_jours_candidature_resultat_final INTEGER,\n",
    "            score_adequation_poste_profil TEXT,\n",
    "            priorite_offre TEXT,\n",
    "            mots_cles_poste TEXT,\n",
    "            motivation TEXT\n",
    "        )\n",
    "        \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24a33-1dd6-4d65-9d05-ad203954dbc0",
   "metadata": {},
   "source": [
    "### Sauvegarde en base PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "id": "e4f8714f-8ab5-43c6-8e03-98bce597c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_postgres_upsert_initial_api(df, engine, table_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un DataFrame pandas dans PostgreSQL avec UPSERT.\n",
    "    Met √† jour last_updated pour chaque ligne ins√©r√©e ou modifi√©e.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): donn√©es √† ins√©rer\n",
    "        engine (sqlalchemy.Engine): moteur SQLAlchemy connect√© √† PostgreSQL\n",
    "        table_name (str): nom de la table cible\n",
    "    \n",
    "    Returns:\n",
    "        int: nombre de lignes ins√©r√©es ou mises √† jour\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.info(\"üì≠ DataFrame vide, rien √† ins√©rer.\")\n",
    "        return 0\n",
    "\n",
    "    # Nettoyage des NaN\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    now = datetime.utcnow()\n",
    "    count = 0\n",
    "\n",
    "    try:        \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # Calcul embedding uniquement si nouvelle offre\n",
    "                if not row.get(\"embedding\"):\n",
    "                    row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'origine_annonce': stmt.excluded.origine_annonce,\n",
    "                        'source': stmt.excluded.source,\n",
    "                        'recherche':stmt.excluded.recherche,\n",
    "                        'titre': stmt.excluded.titre,\n",
    "                        'description': stmt.excluded.description,\n",
    "                        'entreprise': stmt.excluded.entreprise,\n",
    "                        'lieu': stmt.excluded.lieu,\n",
    "                        'latitude': stmt.excluded.latitude,\n",
    "                        'longitude': stmt.excluded.longitude,   \n",
    "                        'commune': stmt.excluded.commune,   \n",
    "                        'code_postal': stmt.excluded.code_postal,   \n",
    "                        'departement': stmt.excluded.departement,                 \n",
    "                        'type_contrat_libelle': stmt.excluded.type_contrat_libelle,\n",
    "                        'date_publication': stmt.excluded.date_publication,\n",
    "                        'url': stmt.excluded.url,\n",
    "                        'secteur_activites': stmt.excluded.secteur_activites,\n",
    "                        # forc√© √† chaque ex√©cution, m√™me sans changement d'autres colonnes\n",
    "                        'last_updated': now,\n",
    "                        'embedding': stmt.excluded.embedding\n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        logging.info(f\"{count} offres ins√©r√©es/mises √† jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"‚ùå Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac263cb7-f4a0-417f-851c-f2690e689095",
   "metadata": {},
   "source": [
    "### Calcul similarit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "id": "3080956b-6445-497c-abb3-a8e5b660d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_similarity(df, engine, table_name):\n",
    "    \"\"\"\n",
    "    Sauvegarde un DataFrame pandas dans PostgreSQL avec UPSERT.\n",
    "    Met √† jour last_updated pour chaque ligne ins√©r√©e ou modifi√©e.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): donn√©es √† ins√©rer\n",
    "        engine (sqlalchemy.Engine): moteur SQLAlchemy connect√© √† PostgreSQL\n",
    "        table_name (str): nom de la table cible\n",
    "    \n",
    "    Returns:\n",
    "        int: nombre de lignes ins√©r√©es ou mises √† jour\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.info(\"üì≠ DataFrame vide, rien √† ins√©rer.\")\n",
    "        return 0\n",
    "\n",
    "    # Nettoyage des NaN\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    metadata = MetaData()\n",
    "    table = Table(table_name, metadata, autoload_with=engine)\n",
    "    now = datetime.utcnow()\n",
    "    count = 0\n",
    "    \n",
    "    print(\"Lancement MAJ du score de similarit√© dans la base !\")\n",
    "    \n",
    "    try:        \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # Calcul embedding uniquement si nouvelle offre\n",
    "                if not row.get(\"embedding\"):\n",
    "                    row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'similitude': stmt.excluded.similitude, \n",
    "                        # forc√© √† chaque ex√©cution, m√™me sans changement d'autres colonnes\n",
    "                        'last_updated': now,\n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        logging.info(f\"{count} offres ins√©r√©es/mises √† jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"‚ùå Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "id": "0f9b0410-dbbc-4748-92c2-ced809ec187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(reference_text, engine, table_name):\n",
    "    ref_emb = compute_embedding(reference_text)\n",
    "    ref_emb_str = \"[\" + \",\".join(map(str, ref_emb)) + \"]\"  # convertir en string pour PGVector\n",
    "\n",
    "    query = text(f\"\"\"\n",
    "        SELECT     \n",
    "                id, origine_annonce, source, recherche, titre, description, entreprise, \n",
    "                lieu, latitude, longitude, commune, code_postal, departement, \n",
    "                type_contrat_libelle, date_publication, url, secteur_activites, \n",
    "                last_updated, embedding, similitude, candidature_envisagee, type_contrat, \n",
    "                experience_requise, candidature_effectuee, date_candidature, \n",
    "                nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "                date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "                etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "                nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "                score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "                motivation, simil_temp\n",
    "        FROM (\n",
    "                SELECT \n",
    "                    id, origine_annonce, source, recherche, titre, description, entreprise, \n",
    "                    lieu, latitude, longitude, commune, code_postal, departement, \n",
    "                    type_contrat_libelle, date_publication, url, secteur_activites, \n",
    "                    last_updated, embedding, similitude, candidature_envisagee, type_contrat, \n",
    "                    experience_requise, candidature_effectuee, date_candidature, \n",
    "                    nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "                    date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "                    etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "                    nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "                    score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "                    motivation, 1 - (embedding <#> (:ref)::vector) AS simil_temp\n",
    "                FROM {table_name}\n",
    "            ) AS s\n",
    "        ORDER BY simil_temp DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {\"ref\": ref_emb_str})       \n",
    "        df_similarity = pd.DataFrame(result.fetchall(),columns=[\"id\", \"origine_annonce\", \"source\", \"recherche\", \"titre\", \n",
    "                                                                \"description\", \"entreprise\", \"lieu\", \"latitude\", \"longitude\", \n",
    "                                                                \"commune\", \"code_postal\", \"departement\", \"type_contrat_libelle\", \n",
    "                                                                \"date_publication\", \"url\", \"secteur_activites\", \"last_updated\", \n",
    "                                                                \"embedding\", \"similitude\", \"candidature_envisagee\", \"type_contrat\", \n",
    "                                                                \"experience_requise\", \"candidature_effectuee\", \"date_candidature\", \n",
    "                                                                \"nom_cv\", \"nom_lm\", \"nom_fichier_offre\", \"date_relance_prevue\", \n",
    "                                                                \"date_relance_effectuee\", \"reponse_recue\", \"date_reponse_entreprise\", \n",
    "                                                                \"etape_atteinte\", \"nom_coord_recruteur\", \"notes_perso\", \"resultat_final\", \n",
    "                                                                \"nb_jours_candidature_reponse\", \"nb_jours_candidature_resultat_final\", \n",
    "                                                                \"score_adequation_poste_profil\", \"priorite_offre\", \"mots_cles_poste\", \n",
    "                                                                \"motivation\",\"simil_temp\"])       \n",
    "        df_similarity['similitude'] = df_similarity['simil_temp']\n",
    "        df_similarity.drop(columns=['simil_temp'], inplace=True)\n",
    "        update_similarity(df_similarity, engine, table_name)\n",
    "        print(\"Le score de similarit√© a √©t√© mis √† jour dans la BDD !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75112a-913c-4cc2-bdfd-f5a320ac2e9d",
   "metadata": {},
   "source": [
    "### Sauvegarde CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "id": "75d0bd1a-552d-4f1a-a5d5-6c71331c54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Sauvegarde Parquet ---\n",
    "# def save_to_csv(df, csv_directory, filename):\n",
    "#     if df.empty:\n",
    "#         return\n",
    "#     os.makedirs(csv_directory, exist_ok=True)\n",
    "#     today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#     path_csv = os.path.join(csv_directory, f\"{today}_{filename}.csv\")\n",
    "#     df.to_csv(path_csv, index=False,encoding=\"utf-8\")\n",
    "#     print(f\"‚úÖ Sauvegard√© dans {path_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c69b5-e661-49b9-be53-abe6835a3a74",
   "metadata": {},
   "source": [
    "### Sauvegarde Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "id": "0b972c51-3307-49aa-a409-21b662763b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Sauvegarde Parquet ---\n",
    "# def save_to_parquet(df,parquet_directory, filename):\n",
    "#     if df.empty:\n",
    "#         return\n",
    "#     os.makedirs(parquet_directory, exist_ok=True)\n",
    "#     today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "#     path_parquet = os.path.join(parquet_directory, f\"{today}_{filename}.parquet\")\n",
    "#     df.to_parquet(path_parquet, index=False)\n",
    "#     print(f\"‚úÖ Sauvegard√© dans {path_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14139dd5-4bb0-445d-bd53-56bbb4dfffe0",
   "metadata": {},
   "source": [
    "### Sauvegarde Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "id": "532f27d3-465d-4937-88cd-decd76bf6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sauvegarde Excel --- \n",
    "def save_to_excel(df,excel_directory, filename):\n",
    "    if df.empty:\n",
    "        return\n",
    "    os.makedirs(excel_directory, exist_ok=True)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    path_excel = os.path.join(excel_directory, f\"{today}_{filename}.xlsx\")\n",
    "    df.to_excel(path_excel, index=False)\n",
    "    return path_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "id": "91b5dcc3-4df3-4bc1-b7f9-8106630cb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_from_table_to_excel(engine, table_name,excel_directory, filename):\n",
    "    query = text(f\"\"\"\n",
    "        SELECT     \n",
    "               id, origine_annonce, source, recherche, titre, \n",
    "               description, entreprise, lieu, latitude, longitude,\n",
    "               commune, code_postal, departement,type_contrat_libelle,\n",
    "               date_publication, url, secteur_activites,last_updated,\n",
    "               embedding, similitude, candidature_envisagee, type_contrat, \n",
    "               experience_requise, candidature_effectuee, date_candidature, \n",
    "               nom_cv, nom_lm, nom_fichier_offre, date_relance_prevue, \n",
    "               date_relance_effectuee, reponse_recue, date_reponse_entreprise, \n",
    "               etape_atteinte, nom_coord_recruteur, notes_perso, resultat_final, \n",
    "               nb_jours_candidature_reponse, nb_jours_candidature_resultat_final, \n",
    "               score_adequation_poste_profil, priorite_offre, mots_cles_poste, \n",
    "               motivation\n",
    "        FROM {table_name}\n",
    "    \"\"\")\n",
    "    \n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query)       \n",
    "        df_export = pd.DataFrame(result.fetchall(),columns=[\"id\", \"origine_annonce\", \"source\", \"recherche\", \"titre\", \n",
    "                                                                \"description\", \"entreprise\", \"lieu\", \"latitude\", \"longitude\", \n",
    "                                                                \"commune\", \"code_postal\", \"departement\", \"type_contrat_libelle\", \n",
    "                                                                \"date_publication\", \"url\", \"secteur_activites\", \"last_updated\", \n",
    "                                                                \"embedding\", \"similitude\", \"candidature_envisagee\", \"type_contrat\", \n",
    "                                                                \"experience_requise\", \"candidature_effectuee\", \"date_candidature\", \n",
    "                                                                \"nom_cv\", \"nom_lm\", \"nom_fichier_offre\", \"date_relance_prevue\", \n",
    "                                                                \"date_relance_effectuee\", \"reponse_recue\", \"date_reponse_entreprise\", \n",
    "                                                                \"etape_atteinte\", \"nom_coord_recruteur\", \"notes_perso\", \"resultat_final\", \n",
    "                                                                \"nb_jours_candidature_reponse\", \"nb_jours_candidature_resultat_final\", \n",
    "                                                                \"score_adequation_poste_profil\", \"priorite_offre\", \"mots_cles_poste\", \n",
    "                                                                \"motivation\"])       \n",
    "        save_to_excel(df_export,excel_directory, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f5499-2879-4fd3-ba43-9ee1bdeac808",
   "metadata": {},
   "source": [
    "### Mise √† jour du fichier de suivi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "id": "b38b8736-ad19-4600-ba7b-218a8d31955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tracking_file_scrapping(engine, tracking_file_path_dir,filename, table_name):   \n",
    "    try:\n",
    "        # Charger ton fichier de suivi existant\n",
    "        os.makedirs(tracking_file_path_dir, exist_ok=True)\n",
    "        tracking_file_path = os.path.join(tracking_file_path_dir, f\"{filename}.xlsx\")\n",
    "        df = pd.read_excel(tracking_file_path)     \n",
    "\n",
    "        if df.empty:\n",
    "            logging.info(\"üì≠ DataFrame vide, rien √† ins√©rer.\")\n",
    "            return 0\n",
    "        \n",
    "        # Nettoyage des NaN\n",
    "        df = df.where(pd.notnull(df), None)       \n",
    "        \n",
    "        metadata = MetaData()\n",
    "        table = Table(table_name, metadata, autoload_with=engine)\n",
    "        now = datetime.utcnow()\n",
    "        count = 0\n",
    "     \n",
    "        with engine.begin() as conn:\n",
    "            for row in df.to_dict(orient=\"records\"):\n",
    "                row[\"last_updated\"] = now\n",
    "                \n",
    "                # # Calcul embedding uniquement si nouvelle offre\n",
    "                # if not row.get(\"embedding\"):\n",
    "                #     row[\"embedding\"] = compute_embedding(row[\"description\"])\n",
    "                    \n",
    "                stmt = pg_insert(table).values(row)\n",
    "                stmt = stmt.on_conflict_do_update(\n",
    "                    index_elements=['id'],\n",
    "                    set_={\n",
    "                        'candidature_envisagee' : stmt.excluded.candidature_envisagee,\n",
    "                        'type_contrat' : stmt.excluded.type_contrat,\n",
    "                        'experience_requise' : stmt.excluded.experience_requise,\n",
    "                        'candidature_effectuee' : stmt.excluded.candidature_effectuee,\n",
    "                        'date_candidature' : stmt.excluded.date_candidature,\n",
    "                        'nom_cv' : stmt.excluded.nom_cv,\n",
    "                        'nom_lm' : stmt.excluded.nom_lm,\n",
    "                        'nom_fichier_offre' : stmt.excluded.nom_fichier_offre,\n",
    "                        'date_relance_prevue' : stmt.excluded.date_relance_prevue,\n",
    "                        'date_relance_effectuee' : stmt.excluded.date_relance_effectuee,\n",
    "                        'reponse_recue' : stmt.excluded.reponse_recue,\n",
    "                        'date_reponse_entreprise' : stmt.excluded.date_reponse_entreprise,\n",
    "                        'etape_atteinte' : stmt.excluded.etape_atteinte,\n",
    "                        'nom_coord_recruteur' : stmt.excluded.nom_coord_recruteur,\n",
    "                        'notes_perso' : stmt.excluded.notes_perso,\n",
    "                        'resultat_final' : stmt.excluded.resultat_final,\n",
    "                        'nb_jours_candidature_reponse' : stmt.excluded.nb_jours_candidature_reponse,\n",
    "                        'nb_jours_candidature_resultat_final' : stmt.excluded.nb_jours_candidature_resultat_final,\n",
    "                        'score_adequation_poste_profil' : stmt.excluded.score_adequation_poste_profil,\n",
    "                        'priorite_offre' : stmt.excluded.priorite_offre,\n",
    "                        'mots_cles_poste' : stmt.excluded.mots_cles_poste,\n",
    "                        'motivation' : stmt.excluded.motivation,\n",
    "                        'last_updated': now       \n",
    "                    }\n",
    "                )\n",
    "                conn.execute(stmt)\n",
    "                count += 1\n",
    "        print(f\"‚úÖ BDD mise √† jour √† partir du fichier de suivi.\")\n",
    "        logging.info(f\"{count} offres ins√©r√©es/mises √† jour dans PostgreSQL.\")\n",
    "        return count\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"‚ùå Erreur lors de l'UPSERT : {str(e)}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdda02-d6b6-40a2-bfac-934fc24cf646",
   "metadata": {},
   "source": [
    "### Pipeline principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "id": "fb6f8148-0086-47ea-b620-5d32c006c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_web_scrapping(query):\n",
    "    logging.info(\"D√©but du pipeline WEB_SCRAPPING.\")\n",
    "\n",
    "    try:        \n",
    "        print(\"Authentification France Travail...\")\n",
    "        token = get_ft_token()\n",
    "    \n",
    "        print(\"R√©cup√©ration des offres France Travail...\")\n",
    "        ft_jobs = fetch_france_travail_jobs(query, token)\n",
    "    \n",
    "        print(\"R√©cup√©ration des offres Adzuna...\")\n",
    "        adzuna_jobs = fetch_adzuna_jobs(query)\n",
    "    \n",
    "        print(\"Fusion et d√©duplication...\")\n",
    "        all_jobs = ft_jobs + adzuna_jobs\n",
    "    \n",
    "        if not all_jobs:\n",
    "            print(\"‚ö†Ô∏è Aucune offre trouv√©e.\")\n",
    "            return\n",
    "    \n",
    "        print(f\"Nombre d'offres d'emploi avant d√©duplication : {len(all_jobs)}\")\n",
    "        jobs_clean = deduplicate(all_jobs)\n",
    "        print(f\"Nombre d'offres d'emploi apr√®s d√©duplication : {len(jobs_clean)}\")\n",
    "    \n",
    "        print(\"Affichage des offres...\")\n",
    "        df = pd.DataFrame(jobs_clean)\n",
    "\n",
    "        print(\"Ajout commune, code_postal et departement...\")\n",
    "        df = get_localization_info(df)\n",
    "    \n",
    "        # Connexion DB\n",
    "        print(\"Connexion √† la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "        # Initier la bdd\n",
    "        api_table_name = 'web_scrapping_table'\n",
    "        init_db(engine, api_table_name)\n",
    "    \n",
    "        print(\"üíæ Sauvegarde en base PostgreSQL...\")\n",
    "        save_to_postgres_upsert_initial_api(df, engine, table_name=api_table_name)\n",
    "        print(\"üíæ Sauvegarde en base PostgreSQL TERMINEE !!!...\")\n",
    "\n",
    "        print(\"üíæ Sauvegarde du score de similarit√© en base PostgreSQL...\")\n",
    "        compute_similarity(reference_text_clean, engine, api_table_name)\n",
    "        print(\"üíæ Sauvegarde du score de similarit√© en base PostgreSQL TERMINEE !!!...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT DE WEB SCRAPPING !!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline √©chou√©: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed20396-5cbd-428b-a2ae-94b941cc0ad2",
   "metadata": {},
   "source": [
    "### Pipeline EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "f1090b6f-f40d-496e-9cbe-bf1ff348f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_export_all(table_name):\n",
    "    logging.info(\"D√©but du pipeline EXPORT.\")\n",
    "\n",
    "    try:           \n",
    "        # Connexion DB\n",
    "        print(\"Connexion √† la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "    \n",
    "        # Initier la bdd\n",
    "        init_db(engine, table_name)\n",
    "\n",
    "        print(\"üíæ Export de la BDD...\")\n",
    "        export_from_table_to_excel(engine, table_name,EXPORT_TRACKING_DIR_PROC, \"export_base\")\n",
    "        print(\"üíæ Export de la BDD TERMIN√©...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT D'EXPORTATION!!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline √©chou√©: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1182481-e737-4f27-8af8-1deb5bf78423",
   "metadata": {},
   "source": [
    "### Pipeline UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "id": "f7ee3f22-eb4d-4723-a12c-93b94573e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- PIPELINE ----------------\n",
    "def run_pipeline_update_all(table_name):\n",
    "    logging.info(\"D√©but du pipeline UPDATE.\")\n",
    "\n",
    "    try:           \n",
    "        # Connexion DB\n",
    "        print(\"Connexion √† la base PostgreSQL...\")\n",
    "        engine = create_engine(f\"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "        # Initier la bdd\n",
    "        init_db(engine, table_name)\n",
    "\n",
    "        tracking_file_path_dir = IMPORT_TRACKING_DIR_PROC\n",
    "        filename = \"update_file\"\n",
    "\n",
    "        print(\"üíæ Mise √† jour de la BDD...\")\n",
    "        update_tracking_file_scrapping(engine, tracking_file_path_dir,filename,table_name)\n",
    "        print(\"üíæ Mise √† jour de la BDD TERMINEE...\")\n",
    "\n",
    "        print(\"FIN DU SCRIPT D'UPDATE!!!...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Pipeline √©chou√©: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "id": "2de560b5-c04e-4118-9662-7069e1da4bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chargement depuis CSV\n",
    "# today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# path_parquet = os.path.join(PARQUET_DIR, f\"{today}_offres.parquet\")\n",
    "# df = pd.read_parquet(path_parquet)\n",
    "# display(df.shape)\n",
    "# display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c859727e-da41-42a1-9772-fa2708d9d558",
   "metadata": {},
   "source": [
    "### Proc√©dure principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "id": "409cc18f-c203-48fd-9849-4feca34d08aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion √† la base PostgreSQL...\n",
      "üíæ Mise √† jour de la BDD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2101/1501279494.py:16: SAWarning: Did not recognize type 'vector' of column 'embedding'\n",
      "  table = Table(table_name, metadata, autoload_with=engine)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BDD mise √† jour √† partir du fichier de suivi.\n",
      "üíæ Mise √† jour de la BDD TERMINEE...\n",
      "FIN DU SCRIPT D'UPDATE!!!...\n",
      "Connexion √† la base PostgreSQL...\n",
      "üíæ Export de la BDD...\n",
      "üíæ Export de la BDD TERMIN√©...\n",
      "FIN DU SCRIPT D'EXPORTATION!!!...\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    JOB_QUERY = [\"data analyst\"]\n",
    "    api_table_name = 'web_scrapping_table'\n",
    "    \n",
    "    for query in JOB_QUERY:\n",
    "        # run_pipeline_web_scrapping(query)\n",
    "        run_pipeline_update_all(api_table_name)\n",
    "        run_pipeline_export_all(api_table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
